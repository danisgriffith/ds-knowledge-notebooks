{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLD-OUT\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "Before we dive deeper into the details of the hold-out evaluation method, let’s take a look at a visual summary of the whole process:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hold-out-summary.png\" alt=\"hold-out\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "<br>\n",
    "First, we <b>randomly split our data into two subsets : a training set and a test set</b>. Setting test data aside is a work-around for dealing with the imperfections of a non-ideal world, such as limited data and resources, and the inability to collect more data from the generating distribution. \n",
    "\n",
    "<br>\n",
    "<b>The test set represents new, unseen data to our learning algorithm; it’s important that we only touch the test set once to make sure we don’t introduce any bias</b> when we estimate the generalization accuracy. The most frequent train-test ratio is 2/3, other common ratios are 60/40, 70/30, 80/20, or even 90/10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hold-out-1.png\" alt=\"hold-out\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "<br>\n",
    "After having set the test samples aside, we will <b>choose a learning algorithm</b> that we think could be appropriate for the given problem. \n",
    "\n",
    "<br>\n",
    "<b>Hyper-parameters</b> (also called meta-parameters) <b>are the parameters of the learning algorithm itself; in contrast to the actual model parameters, the learning algorithm does not learn the hyper-parameters from the training data</b>, we have to specify these values manually.\n",
    "\n",
    "Since hyperparameters cannot be learned during model fitting, we need some sort of \"external loop\" to optimize them separately; we will see that the hold-out method is ill-suited for the task so, at least for the moment, we have to start with some fixed values (we could use our intuition or the default parameters in case we are using an existing library).\n",
    "\n",
    "Once the learning algorightm has been configured into a model, we can <b>fit the latter on the training set</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hold-out-2.png\" alt=\"hold-out\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "<br>\n",
    "Since our model hasn’t seen the test set before, it should give us a pretty unbiased estimate of its performance on new data, in other words, of its ability to generalize. We use the model to predict the class labels for the test set; <b>the predicted labels (of the test set) will be compared with the correct ones in order to estimate the generalization accuracy of our model</b>.\n",
    "\n",
    "<br>\n",
    "Assuming that the algorithm could learn a better model from more data, the test set we with-held from fitting (so that we could have a less optmistic estimate of the generalization performance) represents valuable data which the algorithm hasn't seen yet. <b>If our model has not reached its capacity, our performance estimate would be pessimistically biased</b>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hold-out-3.png\" alt=\"hold-out\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "<br>\n",
    "Now that we have an estimate of how well our model performs on unseen data, <b>there is no reason for with-holding the test set from the algorithm any longer</b>.\n",
    "\n",
    "Since we assume our samples to be IID, there is no reason to assume the model would perform worse after feeding it all the available data. <b>Generally, the model will have a better generalization performance if the algorithms uses more data</b>, given that it hasn’t reached its capacity yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hold-out-4.png\" alt=\"hold-out\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SETUP : importing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "#import sklearn.linear_model as lm\n",
    "import sklearn.neighbors as nbr\n",
    "import sklearn.metrics as mtr\n",
    "\n",
    "import utilcompute as uc\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SETUP : reading in the datasets\n",
    "\n",
    "data = np.column_stack( (load_iris().data, load_iris().target) )\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'species']\n",
    "\n",
    "#print('df.shape[0] : ', df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING : deleting features\n",
    "\n",
    "to_delete = []\n",
    "cols = [c for c in df.columns.values.tolist() if (c not in to_delete)]\n",
    "df = df[cols]\n",
    "\n",
    "#print('columns : ', df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'species'\n",
    "if (isinstance(target, list)):\n",
    "    features = [c for c in df.columns.values.tolist() if (c not in target)]\n",
    "else:\n",
    "    features = [c for c in df.columns.values.tolist() if (c != target)]\n",
    "\n",
    "#print('features : ', features)\n",
    "#print('target   : ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df : \n",
      "\n",
      "{'petal length': 173.96896536339727,\n",
      " 'petal width': 55.48868864572551,\n",
      " 'sepal length': 264.7457109493044,\n",
      " 'sepal width': 97.111605833803296}\n",
      "\n",
      "df_std : \n",
      "\n",
      "{'petal length': 31.397291650719751,\n",
      " 'petal width': 16.141563956997683,\n",
      " 'sepal length': 7.1031134428332869,\n",
      " 'sepal width': 2.0990386257420881}\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING : features standardization\n",
    "\n",
    "vif_dict = uc.compute_vif(df = df, features = features)\n",
    "print('df : ')\n",
    "print()\n",
    "pprint(vif_dict)\n",
    "\n",
    "print()\n",
    "\n",
    "df_std = uc.standardize(df = df, included = features, excluded = target)\n",
    "\n",
    "vif_dict = uc.compute_vif(df = df_std, features = features)\n",
    "print('df_std : ')\n",
    "print()\n",
    "pprint(vif_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING : vif subset selection [reduces multicollinearity]\n",
    "\n",
    "VIF = False\n",
    "\n",
    "if (VIF):\n",
    "    selected_features = uc.vif_best_subset_selection(\n",
    "        vif_threshold = 5, \n",
    "        df = df_std, \n",
    "        features = features, \n",
    "        level = len(features), \n",
    "        debug = False\n",
    "    )\n",
    "    t = uc.concatenate(features, target)\n",
    "    df_std = df_std[t]\n",
    "    \n",
    "    vif_dict = uc.compute_vif(df = df_std, features = selected_features)\n",
    "    pprint(vif_dict)\n",
    "else:\n",
    "    selected_features = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['petal length' 'petal width' 'sepal length' 'sepal width' 'species']\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING : final setup\n",
    "\n",
    "df = df_std\n",
    "features = selected_features\n",
    "\n",
    "print(df_std.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size   :  120\n",
      "test  set size   :  30\n",
      "\n",
      "seed :  1\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL PARAMETERS \n",
    "\n",
    "train_perc = 0.8\n",
    "delimiter = int(len(df) * train_perc)\n",
    "s = 1\n",
    "\n",
    "print('train set size   : ', delimiter)\n",
    "print('test  set size   : ', (len(df) - delimiter))\n",
    "print()\n",
    "print('seed : ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(s)\n",
    "df_shuffled = df.reindex(np.random.permutation(df.index))    \n",
    "    \n",
    "train = df_shuffled[:delimiter]\n",
    "test = df_shuffled[delimiter:]\n",
    "    \n",
    "#    print('df_shuffled indices : {0} ... {1}'.format(df_shuffled.index.values[:3], df_shuffled.index.values[-3:]))     \n",
    "#    print('train set/fold size : ', len(train))\n",
    "#    print('test  set/fold size : ', len(test))\n",
    "#    print()\n",
    "#    print('train : {0} - {1}'.format(train.index.values.tolist()[:3], train.index.values[-3:]))\n",
    "#    print('test  : {0} - {1}'.format(test.index.values.tolist()[:3], test.index.values[-3:]))\n",
    "#    print()\n",
    "    \n",
    "#model = lm.LogisticRegression()\n",
    "model = nbr.KNeighborsClassifier(n_neighbors = 5)\n",
    "model.fit(train[features], train[target])\n",
    "\n",
    "y_pred_train = model.predict(train[features])\n",
    "y_pred_test = model.predict(test[features])\n",
    "\n",
    "#metrics_train = uc.compute_classification_metrics(y = train[target], y_pred = y_pred_train)\n",
    "#metrics_test = uc.compute_classification_metrics(y = test[target], y_pred = y_pred_test)\n",
    "\n",
    "acc_train = mtr.accuracy_score(y_true = train[target], y_pred = y_pred_train, normalize = True, sample_weight = None)\n",
    "acc_test = mtr.accuracy_score(y_true = test[target], y_pred = y_pred_test, normalize = True, sample_weight = None)\n",
    "\n",
    "#score_train = 1 - metrics_train['ACCURACY'] \n",
    "#score_test = 1 - metrics_test['ACCURACY'] \n",
    "\n",
    "score_train = 1 - acc_train\n",
    "score_test = 1 - acc_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train | err :  0.0333333333333\n",
      "test  | err :  0.0666666666667\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('train | err : ', score_train)\n",
    "print('test  | err : ', score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "<br>\n",
    "<b>The expected prediction error estimated through resubstitution is not a reliable approximation of the actual value</b>,\n",
    "since it introduces a very <b>optimistic bias due to overfitting</b>.\n",
    "\n",
    "<br>\n",
    "<b>Hold-out evaluation is a better alternative</b> to resubstitution evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Further questions/issues</b> :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        the distribution of data points which happen to fall in the training or test set; in other words, how the split is\n",
    "        performed\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        some data points may never appear in the training (or test) set\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the size we choose for the test set \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "An advanced version of the hold-out method uses <b>'stratification'</b> to make sure the target variable is represented consistently (or with approximately equal proportions in case of classificationin) in both subsets.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Note</b> : see Repeated Hold-Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     - the choice of the size for the test set has a lot of influence :\n",
    "#       withholding a large portion of the data as a test set may lead to pessimistically biased estimates,\n",
    "#       while reducing the size of the test set may decrease this overly pessimistic bias, \n",
    "#       the variance of our performance estimates will most likely increase. \n",
    "\n",
    "#     - the more we reduce the size of the test set, the closer we resemble the resubstitution method \n",
    "#       and we'll have a progressively optimistic bias but higher variance, the more we increase it (up to a point)\n",
    "#       and we'll have a progressively pessimistic bias but a lower variance\n",
    "\n",
    "#     - we should find the optimal size of the test set that allows us to to have reasonable values \n",
    "#       for both the estimate of expected prediction error and its variance, \n",
    "#       without decreasing too much the size of the training set.\n",
    "\n",
    "#     - having chosen a good test size will not protect us from the effects\n",
    "#       of the distribution of data-points between training and test set, the data in the test set \n",
    "#       (and therefore also in the training set) should change across iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        Sebastian Raschka - Model evaluation, model selection, and algorithm selection in machine learning - Part I <br>\n",
    "        https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html\n",
    "    </li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
