{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#28B463'>ARTIFICIAL NEURAL NETWORKS\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>Introduction\n",
    "\n",
    "<br>\n",
    "<b>Artificial Neural Networks (ANN)</b> are computing systems inspired by the biological neural networks that constitute animal brains; these networks <b>are based on a collection of highly interconnected processing elements (units, or nodes) called artificial neurons</b>, where each unit takes a number of real-valued inputs (possibly the outputs of other units) and produces a single real-valued output, which is a non-linear function of the sum of the inputs and may itself become the input to many other units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>Biological Neurons\n",
    "\n",
    "<br>\n",
    "Before diving into a more detailed explanation of artificial neural networks, we will first take a look at the basic structure of biological neurons and try to understand what are the foundations for the inspiration (of) and analogy to ANN : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='biological_neuron'>\n",
    "    <img src=\"images/biological_neuron.png\" alt=\"biological neuron\" width=\"50%\" height=\"50%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The typical nerve cell of human brain comprises of four parts :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>dendrites</b> : in a biological neuron, dendrites act as the input vector, as they <b>allow the cell to receive\n",
    "        signals from a large number of neighboring neurons</b>. Each dendrite is able to \"modulate\" the input signals by\n",
    "        increasing or decreasing the ratio of synaptic neurotransmitters to signal chemicals introduced into the dendrite in\n",
    "        response to the synaptic neurotransmitter. A negative multiplication effect can be achieved by transmitting signal\n",
    "        inhibitors along the dendrite in response to the reception of synaptic neurotransmitters\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>soma</b> (or cell body) : in a biological neuron, the soma <b>acts as the summation function</b>. As positive and\n",
    "        negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites, positive and negative\n",
    "        ions are effectively added in summation, by simple virtue of being mixed together in the solution inside the cell body\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>axon</b> (or nerve fiber) :  a long, slender projection of the nerve cell whose function is to <b>transmit\n",
    "        information to different neurons</b>. The axon gets its signal from the summation behavior which occurs inside the soma,\n",
    "        <b>once the solution inside the soma reaches a certain (threshold) potential</b>, the axon carry signals\n",
    "        toward the neighboring neurons in the form of action potentials. Action potentials are discrete electrochemical impulses\n",
    "        that travel rapidly along the axon, starting at the cell body and terminating at points where the axon makes synaptic\n",
    "        contact with target cells\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>synapses</b>  : the <b>points of interconnection of one neuron with other neurons</b>; the amount of signal\n",
    "        transmitted depend upon the strength (synaptic weights) of the connections. The connections can be inhibitory\n",
    "        (decreasing strength) or excitatory (increasing strength) in nature\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>Artificial Neurons\n",
    "\n",
    "<br>\n",
    "An <b>artificial neuron</b> is a mathematical function <b>conceived as a model of biological neurons</b>, and <b>is the elementary unit in an artificial neural network</b>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='artificial_neuron'>\n",
    "    <img src=\"images/artificial_neuron.jpg\" alt=\"artificial neuron\" width=\"45%\" height=\"45%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "An input neuron has no predecessor but serves as input interface for the whole network; similarly, an output neuron has no successor and thus serves as output interface of the whole network. Five major components make up an artificial neuron, these components are valid whether the neuron is an input, output, or hidden unit :  \n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>input signals</b> : the artificial neuron receives one or more input signals, corresponding to excitatory/inhibitory\n",
    "        postsynaptic potentials at neural dendrites \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>weights</b> : <b>adaptive coefficients that determine the intensity of the input signal</b> as registered by the\n",
    "        artificial neuron; they represent the strength of the interconnection between the emitter (presynaptic) and the current\n",
    "        neuron (postsynaptic), and can be modified in response to various training sets and according to the network specific\n",
    "        topology or its learning rules\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>summation function</b> : generally, <b>the total input signal is the dot product of the original input vector and the\n",
    "        corresponding weights</b>, but the summation function can be more complex. The input and weighting coefficients can be\n",
    "        combined in many different ways before passing on to the activation function; the specific algorithm for combining\n",
    "        the neural inputs is determined by the chosen network architecture and paradigm. <b>The result of the summation function\n",
    "        is often called \"activation\"</b> and corresponds to the solution formed in the soma\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>activation function</b> : usually, the results of the summation function is passed through a non-linear function\n",
    "        known as an activation function (or transfer function); <b>activation functions are extremely important because they\n",
    "        basically decide </b>whether a neuron should be activated or not, <b>whether or not to transmit the signal to the next\n",
    "        layer of the network </b>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>scaling and limiting functions</b>: after the activation function, the result can pass through additional processes.\n",
    "        Scaling simply multiplies the output by a scale factor and then adds an offset. Limiting is the mechanism which insures\n",
    "        that the scaled result does not exceed upper or lower bounds (in addition to the hard limits that may have been imposed\n",
    "        by the activation function) \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>output function</b> : each processing node is allowed one output signal, which may be emitted to a large number of\n",
    "        other units. Normally, the output is directly equivalent to the result of the activation function; some network\n",
    "        topologies, however, modify the activation output to incorporate competition among neighboring neurons\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "While ANNs are loosely motivated by biological neural systems, <b>there are many complexities to biological neural systems that are not modeled by ANN</b>, and many features of the ANN we will discuss in this notebook are known to be inconsistent with biological systems. For example, we consider here ANN whose individual units output a single constant value, whereas biological neurons output a complex time series of spikes. \n",
    "\n",
    "<br>\n",
    "Historically, <b>two groups</b> of researchers have worked with artificial neural networks: one group has been motivated by the goal of <b>using ANN to study and model biological learning processes</b>, and a second group has been motivated by the goal of <b>obtaining highly effective machine learning algorithms</b>, independent of whether these algorithms mirror biological processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>Architecture of Artificial Neural Networks\n",
    "\n",
    "<br>\n",
    "A typical neural network contains a large number of artificial neurons arranged in a series of layers : signals travel from the first (input), to the last (output) layer, possibly after traversing multiple (hidden) layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='neural_netowrk_architecture_1'>\n",
    "    <img src=\"images/neural_netowrk_architecture_1.jpg\" alt=\"architecture\" width=\"40%\" height=\"40%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Most neural networks are fully connected that means to say each hidden neuron is fully connected to the every neuron in its previous layer and to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='neural_netowrk_architecture_2'>\n",
    "    <img src=\"images/neural_netowrk_architecture_2.jpg\" alt=\"popular architectures\" width=\"45%\" height=\"45%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>Appropriate Problems\n",
    "\n",
    "<br>\n",
    "ANN learning is well-suited to problems in which the training data corresponds to noisy, complex sensor data, such as inputs from cameras or microphones\n",
    "<br>\n",
    "Although a variety of decision tree learning methods have been developed (with somewhat differing capabilities and requirements), decision trees are generally best suited to problems with the following characteristics :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>instances are represented by many attribute-value pairs</b> : as in the case of decision trees, instances can be\n",
    "        described by a vector of predefined features. These input attributes may be highly correlated or independent of one\n",
    "        another; input values can be any real values.\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>the target variable may be discrete-valued, real-valued, or a vector of several (real or discrete-valued)\n",
    "        attributes</b> \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>the training data may contain errors</b> : ANN learning methods are quite robust to noise in the training data\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>long training times are acceptable</b> : network training algorithms typically require longer training times than,\n",
    "        for example, decision tree learning algorithms. Training times can range from a few seconds to many hours, depending\n",
    "        on factors such as the number of weights in the network, the number of training examples considered, and the settings of\n",
    "        various parameters\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>fast evaluation of the learned target function may be required</b> : although ANN learning times are relatively long,\n",
    "        evaluating the learned network in order to apply it to a subsequent instance is typically very fast\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>the ability of humans to understand the learned target function is not important</b> : the weights learned by neural\n",
    "        networks are often difficult for humans to interpret\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        Wikipedia - Artificial Neural Network <br>\n",
    "        https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Wikipedia - Artificial Neuron <br>\n",
    "        https://en.wikipedia.org/wiki/Artificial_neuron\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Bharath University - Charudatta Bangal <br>\n",
    "        Automatic generation control of interconnected power systems using artificial neural network techniques -\n",
    "        Chapter 4 : Artificial Neural Networks <br>\n",
    "        http://shodhganga.inflibnet.ac.in/handle/10603/48 <br>\n",
    "        http://shodhganga.inflibnet.ac.in/bitstream/10603/48/6/chaper%204_c%20b%20bangal.pdf\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Analytics Vidhya - Fundamentals of Deep Learning : Activation Functions and When to Use Them <br>\n",
    "        https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Medium - XenonStack - Overview and Applications of Artificial Neural Networks <br>\n",
    "        https://medium.com/@xenonstack/overview-of-artificial-neural-networks-and-its-applications-2525c1addff7\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Tom Mitchell - Machine Learning <br>\n",
    "        http://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf\n",
    "    </li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
