{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCEPTRON RULE\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "Although we may be more interested in neural networks of many interconnected units, let's begin by understanding how to update the weights for a single node : in this simple case, the learning problem is to determine a weight vector that causes the neuron to produce the correct output for each of the given training examples.\n",
    "\n",
    "<br>\n",
    "Several algorithms are known to solve this learning problem, but in this notebook we are going to consider two of them : <b>the perceptron rule and the delta rule</b>. These two algorithms are guaranteed to converge to somewhat different acceptable hypotheses, under somewhat different conditions, but they are both important to ANN because they <b>provide the basis for learning networks of many units</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Rule\n",
    "\n",
    "<br>\n",
    "The perceptron algorithm is about <b>learning the weights for the input signals in order to draw a linear decision boundary</b> that allows us to discriminate between the two linearly separable classes. Rosenblattâ€™s initial perceptron rule is fairly simple and can be summarized by the following steps:\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        initialize the perceptron weights to zero or small random numbers\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>for each training sample</b>, calculate the perceptron output value and <b>update the weights</b> whenever it\n",
    "        misclassifies an instance\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the last routine is then <b>repeated</b>, each time iterating through all training examples, as many times as needed\n",
    "        <b>until the perceptron classifies all training examples correctly</b>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br><br>\n",
    "Weights are modified at each step according to the perceptron training rule, which is defined as follows : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        w_j \\leftarrow w_j + \\Delta w_j\n",
    "        \\qquad \\text{where} \\quad \\Delta w_j = \\eta \\ (\\text{target}^{(i)} - \\text{output}^{(i)}) \\ x^{(i)}_{j}\n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "        \\qquad \\qquad \\qquad \\qquad & [\\textbf{E1}]   \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Here $ \\eta $ is a positive constant (between 0.0 and 1.0) called the learning rate; the role of the learning rate is to moderate the degree to which weights are changed at each step, and is sometimes made to decay as the number of weight-tuning iterations increases.\n",
    "\n",
    "<br>\n",
    "It is important to note that <b>all weights are being updated simultaneously</b>; for a 2-dimensional dataset, we would write the update as $ w \\leftarrow w + \\Delta w $ :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta w_0 \\quad &= \\quad \\eta \\ (\\text{target}^{(i)} - \\text{output}^{(i)})               \\newline\n",
    "        \\Delta w_1 \\quad &= \\quad \\eta \\ (\\text{target}^{(i)} - \\text{output}^{(i)}) \\ x^{(i)}_{1} \\newline\n",
    "        \\Delta w_2 \\quad &= \\quad \\eta \\ (\\text{target}^{(i)} - \\text{output}^{(i)}) \\ x^{(i)}_{1}\n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "<br>\n",
    "Why should this rule converge toward successful weight values? To get an intuitive feel, let's consider some specific cases. Suppose the training example is correctly classified already by the perceptron; in these two scenarios, the weights remain unchanged :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta w_j \\quad &= \\quad \n",
    "        \\begin{cases}\n",
    "            \\eta \\ [(-1) - (-1)] \\ x^{(i)}_{j} &= 0 \n",
    "                \\qquad \\qquad & \\text{if} \\ \\text{target}^{(i)} = \\text{output}^{(i)} = -1 \\\\\n",
    "            \\eta \\ [(+1) - (+1)] \\ x^{(i)}_{j} &= 0 \n",
    "                \\qquad \\qquad & \\text{if} \\ \\text{target}^{(i)} = \\text{output}^{(i)} = +1\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Suppose now that the perceptron miclassifies the training example. In case of a wrong prediction, the weights are being \"pushed\" towards the direction of the positive or negative target class : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta w_j \\quad &= \\quad \n",
    "        \\begin{cases}\n",
    "            \\eta \\ [(+1) - (-1)] \\ x^{(i)}_{j} \\ = \\ \\eta \\ (2) \\ x^{(i)}_{j} \n",
    "                \\quad & \\text{if} \\ \\text{target}^{(i)} = +1 \\quad \\text{and} \\quad \\text{output}^{(i)} = -1 \\\\\n",
    "            \\eta \\ [(-1) - (+1)] \\ x^{(i)}_{j} \\ = \\ \\eta \\ (-2) \\ x^{(i)}_{j} \n",
    "                \\quad & \\text{if} \\ \\text{target}^{(i)} = -1 \\quad \\text{and} \\quad \\text{output}^{(i)} = +1 \\\\\n",
    "        \\end{cases}\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "In the first case, the perceptron miclassifies a positive instance as negative. <b>In order to make the perceptron change its output, the weights must be altered to increase the value of the dot product $(w \\cdot x)$ ; this will increase the chances of the dot product being on the right side of the decision hyperplane and bring the perceptron closer to a correct classification</b>. \n",
    "\n",
    "<br><br>\n",
    "<b>The perceptron training rule can be proven to converge</b> within a finite number of iterations to a weight vector that correctly classifies all training examples, <b>provided the following conditions</b> :\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>the task is linear</b> in nature (or, in other words, the training examples are linearly separable). <b>When the data\n",
    "        set is not linearly separable, convergence is not assured</b>; in this case, we can set a maximum number of iterations\n",
    "        over the training dataset and/or a threshold for the number of tolerated misclassifications\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>a sufficiently small value of the learning rate</b> is used\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        Tom Mitchell - Machine Learning <br>\n",
    "        http://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Sebastian Raschka - Single Layer Neural Networks and Gradient Descent <br>\n",
    "        http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#the-perceptron-learning-rule\n",
    "    </li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
