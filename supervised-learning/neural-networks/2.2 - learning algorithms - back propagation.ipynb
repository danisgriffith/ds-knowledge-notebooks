{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK PROPAGATION\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "Back Propagation is a <b>supervised learning algorithm used in multilayer feed-forward neural networks</b>. Back Propagation is <b>commonly employed with gradient descent optimization</b> to adjust the weight of neurons by calculating the gradient of the loss function. \n",
    "\n",
    "<br>\n",
    "This technique is also sometimes called \"backward propagation of errors\", because the error is calculated at the output and distributed back through the network layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Space\n",
    "\n",
    "<br>\n",
    "Because we are now considering networks with multiple output units (rather than single units as before), we begin by redefining our loss function to the sum of squared errors over all of the network output units : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\operatorname{J} (w) \n",
    "        \\quad &= \\quad \n",
    "            \\frac{1}{2n} \\ \\sum \\ \\sum _{o \\ \\in \\ outputs} (\\text{target}^{(i)}_{o} - \\text{output}^{(i)}_{o})^2\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad & [\\textbf{E1}]\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "One major difference in the case of multilayer networks is that <b>the error surface can have multiple local minima</b>, in contrast to the single-minimum parabolic error surface shown in the notebook regarding the delta rule. Unfortunately, this means that <b>gradient descent is guaranteed only to converge toward some local minimum, and not necessarily the global minimum error</b>. \n",
    "\n",
    "<br>\n",
    "<a id='backpropagation_cost_function_surface'>\n",
    "    <img src=\"images/backpropagation_cost_function_surface.gif\" alt=\"hypothesis space\" width=\"40%\" height=\"40%\">\n",
    "</a>\n",
    "\n",
    "\n",
    "<br>\n",
    "Despite this obstacle, back propagation has been found to produce excellent results in many real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "<br>\n",
    "<b>Despite the lack of assured convergence to the global minimum error, Back Propagation remains a highly effective function approximation method</b> in practice. In many practical applications the problem of local minima has not been found to\n",
    "be as severe as one might fear. \n",
    "\n",
    "<br>\n",
    "To develop some intuition here, consider that networks with large numbers of weights correspond to error surfaces in very high\n",
    "dimensional spaces (one dimension per weight). When gradient descent falls into a local minimum with respect to one of these weights, it will not necessarily be in a local minimum with respect to the other weights. In fact, <b>the more weights in\n",
    "the network, the more dimensions that might provide \"escape routes\" for gradient descent to fall away from the local minimum with respect to this single weight</b>.\n",
    "\n",
    "<br>\n",
    "Gradient descent over the complex error surfaces represented by ANN is still poorly understood, and no methods are known to\n",
    "predict with certainty when local minima will cause difficulties. Common heuristics to attempt to alleviate the problem of local minima include :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>adding momentum</b> to the <a href='#weight_update'>weight-update</a> rule; momentum can sometimes carry the gradient\n",
    "        descent procedure through narrow local minima, though in principle it can also carry it through narrow global minima\n",
    "        into other local minima\n",
    "    </li>    \n",
    "    <li>\n",
    "        <b>using stochastic gradient descent</b> isntead of true gradient descent; as already discussed, the stochastic\n",
    "        approximation of gradient descent effectively descends a different error surface for each training example, relying on\n",
    "        the average of these examples to approximate the gradient with respect to the full training set. These <b>different\n",
    "        error surfaces will typically have different local minima</b>, making it less likely that the process will get stuck in\n",
    "        any one of them\n",
    "    </li>    \n",
    "    <li>\n",
    "        <b>training multiple networks</b> using the same data, but <b>with different initialization weights</b>; if the\n",
    "        different training efforts lead to different local minima, then the network with the best performance over a separate\n",
    "        validation data set can be selected. Alternatively, all networks can be retained and treated as a \"committee\" of\n",
    "        networks whose output is the (possibly weighted) average of the individual network outputs\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "<br>\n",
    "The back propagation algorithm as described here applies to feedforward networks containing a total of three layers of differentiable activation functions (two hidden layersand the output layer), with units at each layer connected to all units from the preceding layer. \n",
    "\n",
    "<br>\n",
    "Let's begin with a notation which lets us refer to the componentes of the network in an unambiguous way :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        $ w_{jk}^{\\ (l)} $ denotes to the weight assigned to the connection from the $ k^{\\rm th} $ neuron in the \n",
    "        $ (l-1)^{\\rm th} $ layer to the $ j^{\\rm th} $ neuron in the $ l^{\\rm th} $ layer; <br>\n",
    "        the vector $ w^{\\ (l)} $ represents the weights assigned to the totality of the connections to the $ l^{\\rm th} $ layer\n",
    "    </li>    \n",
    "    <li>\n",
    "        $ b_{j}^{\\ (l)} $ denotes the bias added to the $ j^{\\rm th} $ neuron in the $ l^{\\rm th} $ layer; this term can be\n",
    "        decomposed into its own weight and (constant) input components, and is usually expressed as part of the dot product\n",
    "        ( $ \\Sigma_{\\ k = 0} w_{jk}^{\\ (l)} $ instead of $ \\Sigma_{\\ k = 1} w_{jk}^{\\ (l)} + b_{j}^{\\ (l)}$ ); the vector \n",
    "        $ b^{\\ (l)}$ represents the weights assigned to the bias vector for the $ l^{\\rm th} $ layer\n",
    "    </li>    \n",
    "    <li>\n",
    "        $ z_{j}^{\\ (l)} $ denotes the net input for the $ j^{\\rm th} $ neuron in the $ l^{\\rm th} $ layer; \n",
    "        the vector $ z^{\\ (l)}$ represents the net inputs associated to each neuron in the $ l^{\\rm th} $ layer\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $ a_{j}^{\\ (l)} $ denotes the value returned when the net input $ z_{j}^{\\ (l)} $ is passed through the activation\n",
    "        function ($ \\operatorname{g} $ in this notebook); the vector $ a^{\\ (l)} $ represents the activations values of the \n",
    "        $ l^{\\rm th} $ layer, which will serve as the input vector for the next layer\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $ \\delta_n $ denotes the error term associated with unit $n$; this term plays a role analogous to the quantity $ (t - o)\n",
    "        $ we used in our earlier discussion of the delta training rule; as we will see later in this notebook, \n",
    "        $ \\delta_n = \\frac{\\partial \\ \\operatorname{J}}{\\partial \\ net_n} $ \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "The learning algorithm can be divided into three phases : <b>initialization, forward propagation, and back-propagation</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis_p50\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "import utilcompute as uc\n",
    "import preprocessing as pre\n",
    "import artificial_neural_networks as ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SETUP : reading in the datasets\n",
    "\n",
    "data = np.column_stack( (load_boston().data, load_boston().target) )\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = [\n",
    "    'CRIM',    # per capita crime rate by town\n",
    "    'ZN',      # proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    'INDUS',   # proportion of non-retail business acres per town\n",
    "    'CHAS',    # Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    'NOX',     # nitric oxides concentration (parts per 10 million)\n",
    "    'RM',      # average number of rooms per dwelling\n",
    "    'AGE',     # proportion of owner-occupied units built prior to 1940\n",
    "    'DIS',     # weighted distances to five Boston employment centres\n",
    "    'RAD',     # index of accessibility to radial highways\n",
    "    'TAX',     # full-value property-tax rate per $10,000\n",
    "    'PTRATIO', # pupil-teacher ratio by town\n",
    "    'B',       # 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    'LSTAT',   # % lower status of the population\n",
    "    'MEDV'     # median value of owner-occupied homes in $1000's\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'MEDV'\n",
    "features = [c for c in df.columns if (c != target)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING (TRAIN + VALIDATION) : features transformation\n",
    "\n",
    "excluded_features = ['CHAS']\n",
    "included_features = [f for f in features if (f not in excluded_features)]\n",
    "\n",
    "df_std = pre.standardize(df = df, included = included_features, excluded = uc.concatenate(excluded_features,target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are still random but differ in range depending on the size of the previous layer of neurons. This provides a controlled initialisation hence the faster and more efficient gradient descent.\n",
    "\n",
    "Assigning the network weights before training sometimes can be treated as a random process, since we do not know anything about the data, so we are not sure how to assign the weights that would work in that particular case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase I : Initialization\n",
    "\n",
    "<br>\n",
    "There are a number of important choices that need to be made when building and training a neural network : which loss function to use, which activation function, how many layers to have, which optimization algorithm is best suited for the network, etc. With so many things that need to be decided, the choice of initial weights may, at first glance, seem like just another relatively minor detail, but we will see that weight initialization can actually have a profound impact on both the convergence rate and final quality of a network.\n",
    "\n",
    "<br>\n",
    "<b>All-Zero</b> \n",
    "\n",
    "<br>\n",
    "We do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and the other half will be negative.\n",
    "\n",
    "<br>\n",
    "A plausible idea then might be to set all the initial weights to zero, which we expect to be the \"best guess\". This would turn out to be a mistake, because <b>if every neuron in the network computes the same output, then they will also all compute the same gradients during back-propagation and undergo the exact same parameter updates</b>. In other words, there is <b>no source of asymmetry</b> among neurons if the weights are initialized to the same value.\n",
    "\n",
    "<br><br>\n",
    "<b>Small Random Numbers</b> \n",
    "\n",
    "<br>\n",
    "We still want the weights to be very close to zero but, as we have argued above, not identically zero. A common solution is to initialize the weights to small random numbers; the idea is that <b>neurons, when given different initial weight vectors, will compute distinct updates and integrate themselves as diverse parts of the full network</b>. \n",
    "\n",
    "<br>\n",
    "<b>Each weight vector is initialized as a random vector sampled from a multi-dimensional gaussian with zero mean and unit standard deviation</b>. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.\n",
    "\n",
    "<br>\n",
    "Warning: it’s not necessarily the case that smaller numbers will work strictly better. Since gradients are proportional to the value of the associated weights, a layer with very small weights will yield (during back propagation) very small gradients; this could greatly diminish the \"gradient signal\" flowing backward through the network, and could become a concern for deep networks.\n",
    "\n",
    "<br><br>\n",
    "<b>Calibrating the Variance</b> \n",
    "\n",
    "<br>\n",
    "One problem with the suggestion above is that <b>the distribution of the outputs from a randomly initialized neuron has a variance that might vanish (or explode) with the number of inputs</b> :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        if the weights start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        if the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be\n",
    "        useful\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "In order to avoid the two extreme situation highlighted above, our aim should be to maintain the variance of the activation values throughout the network; shortly, we will see how <b>normalizing the output of each neuron to a suitable variance makes sure the weights are \"just right\", keeping the signal in a reasonable range of values through many layers</b>. The key concept of variance calibration is to automatically determine the scale of the weights initialization matrix based on the number of input and output neuron at each layer.\n",
    "\n",
    "<br>\n",
    "Consider the inner product $ s = \\sum_i^n w_i x_i $ between the weights and the input vector, which gives the net input of a neuron before the activation function. Let's examine the variance of this term :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{Var}(s) \n",
    "        &= \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & \\text{by definition}\n",
    "        \\newline\n",
    "        &= \\mathrm{Var} \\ \\bigg( \\ \\sum_i^{n_{\\ in}} w_i \\cdot x_i \\ \\bigg)\n",
    "        \\newline\n",
    "        &= \\sum_i^{n_{\\ in}} \\mathrm{Var} \\ (w_i \\cdot x_i)\n",
    "        \\newline\n",
    "        &= \\sum_i^{n_{\\ in}} {\n",
    "              \\operatorname{E}[w_i]^2 \\ \\mathrm{Var} \\ (x_i) \n",
    "            + \\operatorname{E}[x_i]^2 \\ \\mathrm{Var} \\ (w_i) \n",
    "            + \\mathrm{Var} \\ (x_i) \\ \\mathrm{Var} \\ (w_i)\n",
    "            }\n",
    "            & \\text{assuming } \\operatorname{E} \\ [x_i] = \\operatorname{E} \\ [w_i] = 0\n",
    "        \\newline\n",
    "        &= \n",
    "            \\sum_i^{n_{\\ in}} { \\mathrm{Var} \\ (x_i) \\ \\mathrm{Var} \\ (w_i) }\n",
    "            & \\text{assuming } w_i, x_i \\text{ are IID}\n",
    "        \\newline\n",
    "        &= \n",
    "            n_{in} \\ \\mathrm{Var} \\ (w) \\ \\mathrm{Var} \\ (x)\n",
    "            & [\\textbf{E2}] \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "It is easy to see that the variance of the neuron output is a scaled version of the variance of the input; if we want the two quantities to be the same (or, in other words, if we want the keep the signal in a range of values consistent across layers) we have to impose the following condition on the initial weights :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align*}\n",
    "        &\n",
    "        \\begin{aligned}[t]\n",
    "            \\mathrm{Var} \\ (w) & = 1 \\ / \\ n_{in}\n",
    "        \\end{aligned}        \n",
    "        \\newline \\newline\n",
    "        \\quad \\Rightarrow \\quad\n",
    "        & \n",
    "            w \\sim \\mathcal{N} \\ \\big( \\ 0 \\ , \\ \\sqrt{ 1 \\ / \\ n_{in} } \\ \\big)\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E3}] \n",
    "    \\end{align*}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<b>Scaling the standard deviation of the initial weight vector by the square root of the neuron fan-in</b> (the number of inputs) ensures that at the beginning all neurons in the network have approximately the same output distribution; best known as <b>Xavier Initialization</b>, this normalization technique has been proven to improve the rate of convergence.\n",
    "\n",
    "<br>\n",
    "A similar analysis is carried out in <a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\">Understanding the difficulty of training deep feedforward neural networks</a> (<b>Glorot and Bengio</b>) : in this paper, the authors explain that :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        from a forward-propagation point of view, to keep information flowing we would like to have <br>\n",
    "        $ \\mathrm{Var} \\ (w) \\quad = \\quad 1 \\ / \\ n_{in} $\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        from a back-propagation point of view, to keep the variance of the input and output gradient consistent across layers,\n",
    "        we would like to have <br>\n",
    "        $ \\mathrm{Var} \\ (w) \\quad = \\quad 1 \\ / \\ n_{out} $\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "These two constraints can only be satisfied simultaneously if $ n_{in} = n_{out} $. Unless this is actually the case, we should aim to a reasonable compromise of the two conditions. <b>Glorot and Bengio</b> recommend an initialization in the form of the <b>harmonic mean</b> :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align*}\n",
    "        &\n",
    "        \\begin{aligned}[t]\n",
    "            \\mathrm{Var}(w) \\quad = \\quad 2 \\ / \\ (n_{in} + n_{out})\n",
    "        \\end{aligned}        \n",
    "        \\newline \\newline\n",
    "        \\quad \\Rightarrow \\quad\n",
    "        & \n",
    "            w \\sim \\mathcal{N} \\ \\big( \\ 0 \\ , \\ \\sqrt{ 2 \\ / \\ (n_{in} + n_{out}) } \\ \\big)\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad  \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & [\\textbf{E4}] \n",
    "    \\end{align*}\n",
    "$\n",
    "\n",
    "where $ n_{in} $ and $ n_{out} $ are, respectively, the number of units in the previous and next layer. \n",
    "\n",
    "<br>\n",
    "Glorot initialization works pretty well with sigmoidal non-linearities, but the network have difficulties to converge in case of ReLU. A more recent paper on this topic, <a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> (<b>He</b> et al., 2015), <b>derives an initialization specifically for ReLU neurons, which is the current recommendation for use in the specific case</b> of neural networks with ReLU neurons :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align*}\n",
    "        &\n",
    "        \\begin{aligned}[t]\n",
    "            \\mathrm{Var}(w) \\quad = \\quad 2 \\ / \\ n_{in}\n",
    "        \\end{aligned}        \n",
    "        \\newline \\newline\n",
    "        \\quad \\Rightarrow \\quad\n",
    "        & \n",
    "            w \\sim \\mathcal{N} \\ \\big( \\ 0 \\ , \\ \\sqrt{ 2 \\ / \\ n_{in} } \\ \\big)\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E5}] \n",
    "    \\end{align*}\n",
    "$\n",
    "\n",
    "<br><br>\n",
    "<b>Sparse Initialization</b> \n",
    "\n",
    "<br>\n",
    "Another way to address the problem of uncalibrated variances is to set all weight matrices to zero but, in order to break symmetry, every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it. A typical number of neurons to connect to may be as small as 10.\n",
    "\n",
    "<br><br>\n",
    "<b>Initializing Biases</b> \n",
    "\n",
    "<br>\n",
    "It is possible (and common) to initialize the biases to zero, since the asymmetry is provided by the small random numbers in the weights. Setting all biases to a small constant value such as 0.01 ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement (in fact some results seem to indicate that this performs worse) and it is more common to simply use zero bias initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layers = [100,100]\n",
    "\n",
    "weights = []\n",
    "for l in range(0, len(hidden_layers)):    \n",
    "    layer_weight_vector = np.zeros(shape = (hidden_layers[l],1), dtype = float)\n",
    "    weights.append(layer_weight_vector)\n",
    "weights_all_zero = weights\n",
    "\n",
    "weights = []\n",
    "np.random.seed(1)\n",
    "for l in range(1, len(hidden_layers)):    \n",
    "    layer_weight_vector = np.random.randn(hidden_layers[l], hidden_layers[l-1])\n",
    "    weights.append(layer_weight_vector)\n",
    "weights_small_random_number = weights\n",
    "\n",
    "weights = []\n",
    "np.random.seed(1)\n",
    "for l in range(1, len(hidden_layers)):    \n",
    "    factor = np.sqrt(1.0 / hidden_layers[l-1])\n",
    "    layer_weight_vector = np.random.randn(hidden_layers[l], hidden_layers[l-1]) * factor\n",
    "    weights.append(layer_weight_vector)\n",
    "weights_xavier = weights\n",
    "\n",
    "weights = []\n",
    "np.random.seed(1)\n",
    "for l in range(1, len(hidden_layers)):    \n",
    "    factor = np.sqrt(2.0 / hidden_layers[l-1])\n",
    "    layer_weight_vector = np.random.randn(hidden_layers[l], hidden_layers[l-1]) * factor\n",
    "    weights.append(layer_weight_vector)\n",
    "weights_he = weights\n",
    "\n",
    "weights = []\n",
    "np.random.seed(1)\n",
    "for l in range(1, len(hidden_layers)):  \n",
    "    factor = np.sqrt(2.0 / (hidden_layers[l] + hidden_layers[l-1]))\n",
    "    layer_weight_vector = np.random.randn(hidden_layers[l], hidden_layers[l-1]) * factor\n",
    "    weights.append(layer_weight_vector)\n",
    "weights_glorot = weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis_p50\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "max_iter = 50\n",
    "\n",
    "mlp_list = [\n",
    "    { 'df' : df_std, 'module' : nn, 'model_name' : 'MLPRegressor', \n",
    "     'model_args' : { 'hidden_layer_sizes' : (100,100), 'activation' : 'relu', 'solver' : 'sgd', \n",
    "                     'learning_rate' : 'adaptive', 'learning_rate_init' : 0.001, 'max_iter' : max_iter}, \n",
    "     'weights' : weights_all_zero, \n",
    "     'note' : 'all-zero' \n",
    "    },\n",
    "    { 'df' : df_std, 'module' : nn, 'model_name' : 'MLPRegressor', \n",
    "     'model_args' : { 'hidden_layer_sizes' : (100,100), 'activation' : 'relu', 'solver' : 'sgd', \n",
    "                     'learning_rate' : 'adaptive', 'learning_rate_init' : 0.001, 'max_iter' : max_iter}, \n",
    "     'weights' : weights_small_random_number, \n",
    "     'note' : 'small random numbers' \n",
    "    },\n",
    "    { 'df' : df_std, 'module' : nn, 'model_name' : 'MLPRegressor', \n",
    "     'model_args' : { 'hidden_layer_sizes' : (100,100), 'activation' : 'relu', 'solver' : 'sgd', \n",
    "                     'learning_rate' : 'adaptive', 'learning_rate_init' : 0.001, 'max_iter' : max_iter}, \n",
    "     'weights' : weights_xavier, \n",
    "     'note' : 'xavier variance calibration' \n",
    "    },\n",
    "    { 'df' : df_std, 'module' : nn, 'model_name' : 'MLPRegressor', \n",
    "     'model_args' : { 'hidden_layer_sizes' : (100,100), 'activation' : 'relu', 'solver' : 'sgd', \n",
    "                     'learning_rate' : 'adaptive', 'learning_rate_init' : 0.001, 'max_iter' : max_iter}, \n",
    "     'weights' : weights_he, \n",
    "     'note' : 'he variance calibration' \n",
    "    },\n",
    "    { 'df' : df_std, 'module' : nn, 'model_name' : 'MLPRegressor', \n",
    "     'model_args' : { 'hidden_layer_sizes' : (100,100), 'activation' : 'relu', 'solver' : 'sgd', \n",
    "                     'learning_rate' : 'adaptive', 'learning_rate_init' : 0.001, 'max_iter' : max_iter}, \n",
    "     'weights' : weights_glorot, \n",
    "     'note' : 'glorot variance calibration' \n",
    "    } \n",
    "]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for l in mlp_list:\n",
    "    results_temp = ann.compute_kfold_stats(\n",
    "        df = l['df'], \n",
    "        features = features, \n",
    "        target = [target], \n",
    "        module = l['module'],\n",
    "        model_name = l['model_name'], \n",
    "        model_args = l['model_args'],\n",
    "        weights = l['weights'],\n",
    "        note = l['note'],\n",
    "        task = 'regression',\n",
    "        debug = False\n",
    "    )\n",
    "    results = results.append(results_temp).reset_index(drop = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAAEWCAYAAAD4hifnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8820466d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = np.arange(1, max_iter + 1)\n",
    "\n",
    "scoring_list = {'loss_curve' : 'neg_loss_curve'}\n",
    "    #'MSE' : 'neg_mean_squared_error', \n",
    "    #'R2' : 'r2'}\n",
    "\n",
    "ann.plot_scoring_list(\n",
    "    x = iterations, \n",
    "    scoring_list = scoring_list,\n",
    "    discriminant_col = 'note',\n",
    "    results_df = results, \n",
    "    plot_args = {\n",
    "        'xticks' : iterations, 'xscale' : 'linear', 'xlim' : (0.65,8), 'xlabel' : 'iterations', \n",
    "        'title' : 'multilayer perceptron', }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase II : Forward Propagation\n",
    "\n",
    "<br>\n",
    "The architecture depicted in the figure below is a Multi-Layer Perceptron; MLP can be understood as a network of multiple artificial neurons over multiple layers, connected by means of non-linear activation functions which allow the network to create complex, non-linear decision boundaries and tackle a variety of linear and non-linear problems. The first hidden layer will be able to learn very simple patterns, and each additional hidden layer will be able to learn progressively more complicated patterns. \n",
    "\n",
    "<br>\n",
    "When calculating the depth of a deep neural network, we only consider the layers that have weights attached to them; in other words, we usually count the number of hidden layers along with the output layer. The ann in the picture is therefore a three-layers (deep) neural network. \n",
    "\n",
    "<br>\n",
    "<b>Note</b> : the term \"perceptron\" is a little bit unfortunate in this context, since it really doesn’t have much to do with Rosenblatt’s Perceptron algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='forward_propagation'>\n",
    "    <img src=\"images/multi_layer_perceptron.png\" alt=\"forward propagation\" width=\"70%\" height=\"70%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>A neural network generates an output after passing all the inputs across the layers.</b> This process is called <b>forward propagation</b>, and can be summarized in the following steps :\n",
    "\n",
    "<ol>\n",
    "    <li>for each neuron in the first hidden layer :\n",
    "        <br>\n",
    "        <ol type=\"a\">\n",
    "            <li>compute the dot product $ z $ of the weights and input vectors (including bias)</li>\n",
    "            <li>\n",
    "                feed $ z $ into the neuron activation function, the activation values $ a $ will provide the input for the next\n",
    "                hidden layer\n",
    "            </li>\n",
    "        </ol>\n",
    "        <br><br>\n",
    "        $\n",
    "            \\quad\n",
    "            \\begin{bmatrix}\n",
    "                \\ a_{1}^{\\ (1)}  \\ \\\\\n",
    "                \\ a_{2}^{\\ (1)}  \\ \\\\\n",
    "                \\ \\vdots \\ \\\\\n",
    "                \\ a_{4}^{\\ (1)} \\\n",
    "            \\end{bmatrix}_\\textit{ 4 x 1}\n",
    "            \\quad = \\quad\n",
    "            \\begin{bmatrix}\n",
    "                \\ \\operatorname{g} \\ ( \\ z_{1}^{\\ (1)} \\ ) \\ \\\\\n",
    "                \\ \\operatorname{g} \\ ( \\ z_{2}^{\\ (1)} \\ ) \\ \\\\\n",
    "                \\ \\vdots \\ \\\\\n",
    "                \\ \\operatorname{g} \\ ( \\ z_{4}^{\\ (1)} \\ ) \\ \n",
    "            \\end{bmatrix}_\\textit{ 4 x 1}\n",
    "            \\quad = \\quad\n",
    "            \\operatorname{g} \\ \n",
    "            \\begin{bmatrix}\n",
    "                \\  \n",
    "                      W_{10}^{\\ (1)} \\cdot x_{0} \n",
    "                    + W_{11}^{\\ (1)} \\cdot x_{1} \n",
    "                    + W_{12}^{\\ (1)} \\cdot x_{2} \n",
    "                    + W_{13}^{\\ (1)} \\cdot x_{3} \\ \n",
    "                \\\n",
    "                \\\\\n",
    "                \\ \n",
    "                      W_{20}^{\\ (1)} \\cdot x_{0} \n",
    "                    + W_{21}^{\\ (1)} \\cdot x_{1} \n",
    "                    + W_{22}^{\\ (1)} \\cdot x_{2} \n",
    "                    + W_{23}^{\\ (1)} \\cdot x_{3} \\ \n",
    "                \\\n",
    "                \\\\\n",
    "                \\ \\vdots \\\n",
    "                \\\\ \n",
    "                \\ \n",
    "                      W_{40}^{\\ (1)} \\cdot x_{0} \n",
    "                    + W_{41}^{\\ (1)} \\cdot x_{1} \n",
    "                    + W_{42}^{\\ (1)} \\cdot x_{2} \n",
    "                    + W_{43}^{\\ (1)} \\cdot x_{3}\n",
    "                \\\n",
    "            \\end{bmatrix}_\\textit{ 4 x 1}\n",
    "        $        \n",
    "    <br><br>\n",
    "    <li>repeat the above steps for all layers, further propagating the signal towards the output layer</li>\n",
    "    <br><br>\n",
    "    $\n",
    "        \\quad\n",
    "        \\begin{bmatrix}\n",
    "            \\ a_{1}^{\\ (2)}  \\ \\\\\n",
    "            \\ a_{2}^{\\ (2)}  \\ \\\\\n",
    "            \\ \\vdots \\ \\\\\n",
    "            \\ a_{4}^{\\ (2)} \\\n",
    "        \\end{bmatrix}_\\textit{ 4 x 1}\n",
    "        \\quad = \\quad\n",
    "        \\begin{bmatrix}\n",
    "            \\ \\operatorname{g} \\ ( \\ z_{1}^{\\ (2)} \\ ) \\ \\\\\n",
    "            \\ \\operatorname{g} \\ ( \\ z_{2}^{\\ (2)} \\ ) \\ \\\\\n",
    "            \\ \\vdots \\ \\\\\n",
    "            \\ \\operatorname{g} \\ ( \\ z_{4}^{\\ (2)} \\ ) \\ \n",
    "        \\end{bmatrix}_\\textit{ 4 x 1}\n",
    "        \\quad = \\quad\n",
    "        \\operatorname{g} \\ \n",
    "        \\begin{bmatrix}\n",
    "            \\ \n",
    "                  W_{10}^{\\ (2)} \\cdot a_{0}^{\\ (1)} \n",
    "                + W_{11}^{\\ (2)} \\cdot a_{1}^{\\ (1)} \n",
    "                + \\ \\cdots \\\n",
    "                + W_{14}^{\\ (2)} \\cdot a_{4}^{\\ (1)} \\ \n",
    "            \\\n",
    "            \\\\\n",
    "            \\ \n",
    "                  W_{20}^{\\ (3)} \\cdot a_{0}^{\\ (1)} \n",
    "                + W_{21}^{\\ (3)} \\cdot a_{1}^{\\ (1)} \n",
    "                + \\ \\cdots \\\n",
    "                + W_{24}^{\\ (3)} \\cdot a_{4}^{\\ (1)} \\ \n",
    "            \\\n",
    "            \\\\\n",
    "            \\ \\vdots \\\n",
    "            \\\\\n",
    "            \\ \n",
    "                  W_{40}^{\\ (3)} \\cdot a_{0}^{\\ (1)} \n",
    "                + W_{41}^{\\ (3)} \\cdot a_{1}^{\\ (1)} \n",
    "                + \\ \\cdots \\\n",
    "                + W_{44}^{\\ (3)} \\cdot a_{4}^{\\ (1)} \\ \n",
    "            \\\n",
    "        \\end{bmatrix}_\\textit{ 4 x 1}\n",
    "    $\n",
    "    <br><br>\n",
    "    <li>collect the output</li>\n",
    "    <br><br>\n",
    "    $\n",
    "        \\quad\n",
    "        \\begin{bmatrix}\n",
    "            \\ o_{1} \\ \\\\\n",
    "            \\ o_{2} \\ \n",
    "        \\end{bmatrix}_\\textit{ 2 x 1}\n",
    "        \\quad = \\quad\n",
    "        \\begin{bmatrix}\n",
    "            \\ \\operatorname{g} \\ ( \\ z_{1}^{\\ (3)} \\ ) \\ \\\\\n",
    "            \\ \\operatorname{g} \\ ( \\ z_{2}^{\\ (3)} \\ ) \\ \\\\\n",
    "        \\end{bmatrix}_\\textit{ 2 x 1}\n",
    "        \\quad = \\quad\n",
    "        \\operatorname{g} \\\n",
    "        \\begin{bmatrix}\n",
    "            \\ \n",
    "                  W_{10}^{\\ (3)} \\cdot a_{0}^{\\ (2)} \n",
    "                + W_{11}^{\\ (3)} \\cdot a_{1}^{\\ (2)} \n",
    "                + \\ \\cdots \\\n",
    "                + W_{14}^{\\ (3)} \\cdot a_{4}^{\\ (2)} \\ \n",
    "            \\\n",
    "            \\\\\n",
    "            \\ \n",
    "                  W_{20}^{\\ (3)} \\cdot a_{0}^{\\ (2)} \n",
    "                + W_{21}^{\\ (3)} \\cdot a_{1}^{\\ (2)} \n",
    "                + \\ \\cdots \\\n",
    "                + W_{24}^{\\ (3)} \\cdot a_{4}^{\\ (2)} \n",
    "            \\         \n",
    "        \\end{bmatrix}_\\textit{ 2 x 1}\n",
    "    $\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase III : Back Propagation\n",
    "\n",
    "<br>\n",
    "When we started this notebook, we said that <b>Back Propagation a supervised learning algorithm <i>commonly employed with gradient descent</i> to adjust the weights matrix of an artificial neural network</b>, but what does that mean exactly? Which algorithm is responsible for what ? \n",
    "\n",
    "<br>\n",
    "We discussed gradient descent when we first introduced it in the notebook about the delta rule; we know it's a general, iterative optimization algorithm that can be used to search through a large (or infinite) hypothesis space as long as two conditions are met :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        the hypothesis space itself contains continuously parameterized hypotheses (e.g. the weights in a linear unit)\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the error can be differentiated with respect to these hypothesis parameters\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br><br>\n",
    "Let's take a look at the cost function and we may start to see the issue in applying gradient descent on deep neural networks : <b>there is no direct relationship between the cost function $ \\operatorname{J} (w) $ and the weights $ (w) $ </b>.\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\operatorname{J} (w) \n",
    "        &= \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad  & \\text{by } \\textbf{E1}\n",
    "        \\newline\n",
    "        &= \\frac{1}{2n} \\ \\sum \\ \\sum _{o \\ \\in \\ outputs} (\\text{target}^{(i)}_{o} - \\text{output}^{(i)}_{o})^2\n",
    "        \\newline\n",
    "        &= \\frac{1}{2n} \\ \\sum \\ \\bigg[ \\ (y_{1}^{ \\ (i)} - o_{1}^{ \\ (i)})^2 +  (y_{2}^{ \\ (i)} - o_{2}^{ \\ (i)})^2 \\ \\bigg]\n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Recall that <b>by using gradient descent we need to find the gradient of our cost function with respect to (w.r.t.) the weights</b>; in order to find this gradient, we need to compute the partial derivative of the cost with all the variables in the preceding layers, such as the activation values $ a $ and the net inputs $ z $. This is where we need Back Propagation. \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        { \\frac {\\partial \\ J}             {\\partial \\ W_{14}^{\\ (3)}} }\n",
    "        \\quad &= \\quad \n",
    "        { \\frac {\\partial \\ J}             {\\partial \\ a_{1}^{\\ (3)}}  } \\ \\cdot \\\n",
    "        { \\frac {\\partial \\ a_{1}^{\\ (3)}} {\\partial \\ z_{1}^{\\ (3)}}  } \\ \\cdot \\ \n",
    "        { \\frac {\\partial \\ z_{1}^{\\ (3)}} {\\partial \\ W_{14}^{\\ (3)}} }\n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad  \n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E6}] \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<b>Back Propagation is basically a repeated application of chain rule of calculus for partial derivatives, and provides probably the most efficient technique to find the gradient of our cost function w.r.t. all the learnable parameters in the neural network</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Derivative of the error with respect to the activation value</b>\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\frac {\\partial \\ J} {\\partial \\ a_{1}^{\\ (3)}}\n",
    "        \\quad &= \\quad \n",
    "        \\frac {\\partial}{\\partial \\ o_{1} } \\ \\bigg[ \\frac{1}{2n} \\ (y_{1} - o_{1})^2 \\bigg]\n",
    "        \\quad = \\quad \n",
    "        - \\ (y_{1} - o_{1})\n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E6-A}] \n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Derivative of the activation value with respect to the net input</b>\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\frac {\\partial \\ a_{1}^{\\ (3)}} {\\partial \\ z_{1}^{\\ (3)}} \n",
    "        \\quad &= \\quad \n",
    "        \\newline \n",
    "        \\quad &= \\quad \n",
    "            \\frac \n",
    "                {\\partial}\n",
    "                {\\partial \\ z_{1}^{\\ (3)}} \\ \\bigg[ \\ \\frac{1}{1 + e^{ \\ - \\left( \\ z_{1}^{\\ (3)} \\ \\right)}} \\ \\bigg]\n",
    "        \\newline \\newline\n",
    "        \\quad &= \\quad \n",
    "            \\frac \n",
    "                { e^{- \\left( \\ z_{1}^{\\ (3)} \\ \\right)} } \n",
    "                { \\left( \\ 1 + e^{ \\ - \\left( \\ z_{1}^{\\ (3)} \\ \\right)} \\ \\right)^2 }\n",
    "        \\newline \\newline\n",
    "        \\quad &= \\quad \n",
    "               \\left[ \\ 1 - \\frac{1}{ 1 + e^{ \\ - \\left( \\ z_{1}^{\\ (3)} \\ \\right)} } \\ \\right]\n",
    "             \\ \\frac{1}{ 1 + e^{ \\ - \\left( \\ z_{1}^{\\ (3)} \\ \\right)} }\n",
    "        \\newline \\newline\n",
    "        \\quad &= \\quad\n",
    "            ( \\ 1 - a_{1}^{\\ (3)} \\ ) \\ a_{1}^{\\ (3)}\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E6-B}] \n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Derivative of the net input with respect to a weight</b>\n",
    "\n",
    "<br>\n",
    "Please note that the only one term of the net input that will have a non-zero derivative is the one associated with the particular weight we are considering : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\frac {\\partial \\ z_{1}^{\\ (3)}} {\\partial \\ W_{14}^{\\ (3)}} \n",
    "        \\quad &= \\quad \n",
    "        \\newline \n",
    "        \\quad &= \\quad \n",
    "            \\frac {\\partial}{\\partial \\ W_{14}^{\\ (3)}} \n",
    "            \\ \\bigg[ \\ \n",
    "                  W_{10}^{\\ (3)} \\cdot a_{0}^{\\ (2)} \n",
    "                + W_{11}^{\\ (3)} \\cdot a_{1}^{\\ (2)} \n",
    "                + \\ \\cdots \\\n",
    "                + W_{14}^{\\ (3)} \\cdot a_{4}^{\\ (2)} \n",
    "            \\ \\bigg]\n",
    "        \\newline \\newline\n",
    "        \\quad &= \\quad \n",
    "            a_{4}^{\\ (2)}\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E6-C}] \n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b><a id='weight_update'>Weight Update</a></b>\n",
    "\n",
    "<br>\n",
    "Now that we have back-propagated the error term throughout the network and calculated the partial derivative of the cost function w.r.t. every single component (activation values, net inputs, and weights), we can lay out our findings into an explicit change rule, just like we did for the perceptron and delta rules :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta W_{14}^{\\ (3)} \n",
    "        &= \n",
    "        \\newline\n",
    "        &= \n",
    "            - \\ \\eta \\ { \\frac {\\partial \\ J} {\\partial \\ W_{14}^{\\ (3)}} }\n",
    "            & \\text{by } \\textbf{E6}\n",
    "        \\newline\n",
    "        &= \n",
    "            - \\ \\eta \\ \n",
    "            \\left[ \\\n",
    "            { \\frac {\\partial \\ J}             {\\partial \\ a_{1}^{\\ (3)}}  } \\ \\cdot \\\n",
    "            { \\frac {\\partial \\ a_{1}^{\\ (3)}} {\\partial \\ z_{1}^{\\ (3)}}  } \\ \\cdot \\ \n",
    "            { \\frac {\\partial \\ z_{1}^{\\ (3)}} {\\partial \\ W_{14}^{\\ (3)}} } \\ \n",
    "            \\right]\n",
    "            & \\text{by } \\textbf{E6-A, E6-B, E6-C}\n",
    "        \\newline \\newline\n",
    "        &= \n",
    "            \\eta \\\n",
    "            \\Bigg[ \\\n",
    "                \\underbrace\n",
    "                {\n",
    "                    (y_{1} - o_{1}) \\ \\cdot \\\n",
    "                    ( \\ 1 - a_{1}^{\\ (3)} \\ ) \\ a_{1}^{\\ (3)} \n",
    "                }\n",
    "                _{ \\delta_{1}^{(3)} } \n",
    "                \\ \\cdot \\ a_{4}^{\\ (2)}\\\n",
    "            \\Bigg]                         \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & [\\textbf{E7}] \n",
    "    \\end{align}    \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This equation is typically simplified as shown below, where the term $ \\delta $ repesents the product of the error with the derivative of the activation function : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\delta_{j}^{\\ (l)}\n",
    "        \\quad &= \\quad    \n",
    "              \\frac {\\partial \\ J} {\\partial \\ a_{j}^{\\ (l)}}\n",
    "            \\ \\frac {\\partial \\ a_{j}^{\\ (l)}} {\\partial \\ z_{j}^{\\ (l)}} \n",
    "        \\quad = \\quad\n",
    "        \\begin{cases}\n",
    "            (y_{j} - o_{j}) \\ (1 - o_{j}) \\ o_{j}                    & \\text{if } j \\text{ is an output neuron} \\\\\n",
    "            \\\\\n",
    "            (\\sum _{k} \\ \\delta _{k} \\ W_{jk}) \\ (1 - o_{j}) \\ o_{j} & \\text{if } j \\text{ is an inner neuron}\n",
    "        \\end{cases}\n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & [\\textbf{E8}]         \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br><br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta W_{ji} \n",
    "        \\quad &= \\quad \n",
    "            & \\text{by } \\textbf{E6-A, E6-B, E6-C}\n",
    "        \\newline\n",
    "        &=\n",
    "            - \\ \\eta \\ \n",
    "            \\left[ \\ \\sum_{k} \\ \n",
    "            { \\frac {\\partial \\ J}     {\\partial \\ a_{k}} } \\ \\cdot \\\n",
    "            { \\frac {\\partial \\ a_{k}} {\\partial \\ z_{k}} } \\ \\cdot \\ \n",
    "            { \\frac {\\partial \\ z_{k}} {\\partial \\ a_{j}} } \\ \n",
    "            \\right] \\ \n",
    "            { \\frac {\\partial \\ a_{j}} {\\partial \\ z_{j}}  } \\ \\cdot \\ \n",
    "            { \\frac {\\partial \\ z_{j}} {\\partial \\ W_{ji}} } \n",
    "            & \\text{by } \\textbf{E8}\n",
    "        \\newline\n",
    "        &= \\quad \n",
    "            \\eta \\ \n",
    "            \\Bigg[ \\ \\sum_{k} \\ \\underbrace { (a_{k} - y_{k}) \\ (1 - o_{k}) \\ a_{k} } _{\\delta_{k} } \\ W_{kj} \\ \\Bigg] \\ \n",
    "            a_{j} \\  (1 - a_{j}) \\ a_{i} \n",
    "            & \\text{by } \\textbf{E8}\n",
    "        \\newline \n",
    "        &= \\quad \n",
    "            \\eta \\ \n",
    "            \\underbrace{ \\left[ \\ \\sum_{k} \\ \\delta_{k} \\ W_{kj} \\ \\right] \\ a_{j} \\ (1 - a_{j}) }_{\\delta_{j}} \\ a_{i}        \n",
    "        \\newline\n",
    "        &= \\quad \n",
    "            \\eta \\ \\delta_{j} \\ a_{i}\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E9}] \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br><br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta W_{14}^{\\ (3)} \n",
    "        \\quad &= \\quad \n",
    "            \\eta \\ \\delta_{1}^{\\ (3)} \\ a_{4}^{\\ (2)}   \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad & [\\textbf{E10}] \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br><br>\n",
    "<b>Note</b>\n",
    "\n",
    "<br>\n",
    "The algorithm we have described so far updates the weights incrementally, following the presentation of each training example. We know that this version of the algorithm corresponds to a stochastic approximation of the original gradient descent, and that in order to obtain the true gradient of the cost function we would simply have to sum the values $ \\delta_{j} \\ a_{i} $ over all training examples before altering the weight values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Momentum\n",
    "\n",
    "<br>\n",
    "Because Back Propagation is such a widely used algorithm, many variations have been developed. Perhaps the most common is to alter the weight-update rule in <b>E9</b> by making the weight update on the $ n^{th} $ iteration depend partially on the update occurred during the $ (n - 1)^{th} $iteration, as follows :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\Delta W_{ji} (n)\n",
    "        \\quad &= \\quad \n",
    "        \\eta \\ \\delta_{j} \\ a_{i} + \\alpha \\ \\Delta W_{ji} \\ (n - 1)\n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & [\\textbf{E11}] \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "The first term on the right-hand side of the equation is the original weight-update rule, whereas the second term $ 0 \\leq \\alpha < 1 $ is a new constant called momentum. \n",
    "\n",
    "<br>\n",
    "To see the effect of this momentum term, consider that the gradient descent search trajectory is analogous to that of a (momentumless) ball rolling down the error surface. If the weight change carries on in the direction it was going in the previous epoch, then the movement will be a little more pronounced in the current epoch, and this effect will be compounded as the search continues in the same direction. When the trend finally reverses, then the search may be at the global minimum, in which case it is hoped that the momentum won't be enough to take it anywhere other than where it is. Alternatively, the search may be at a fairly narrow local minimum. In this case, even though the backpropagation algorithm dictates that $ \\Delta $ will change direction, it may be that the additional term from the previous epoch (the momentum) may be enough to counteract this effect for a few steps. These few steps may be all that is needed to bypass the local minimum.\n",
    "\n",
    "<br>\n",
    "In addition to getting over some local minima, when the gradient is constant in one direction, adding momentum will increase the size of the weight change after each epoch, and the network may converge quicker. \n",
    "\n",
    "In conclusion, <b>the effect of $ \\alpha $ is to add momentum which tends to keep the ball rolling in the same direction from one iteration to the next. This can sometimes have the effect of keeping the ball rolling through small local minima in the error surface, or along flat regions in the surface where the ball would stop if there were no momentum</b>. \n",
    "\n",
    "<br>\n",
    "Momentum also has the effect of <b>gradually increasing the step size of the search in regions where the gradient is unchanging, thereby speeding convergence</b>.\n",
    "\n",
    "<br><br>\n",
    "<b>Note</b>\n",
    "\n",
    "<br>\n",
    "Please keep in mind that it is possible to have cases where the momentum is not enough to carry the search out of a local minima, or where the momentum carries the search out of the global minima into a local minima. This is why this technique is a heuristic method and should be used somewhat carefully (it is used in practice a great deal).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination\n",
    "\n",
    "<br>\n",
    "Given a fixed network structure like that in the picture below, the main loop of the algorithm repeatedly iterates over the training examples. For each training example, it applies the network to the example, calculates the output error, computes the gradient with respect to the error on this example, then updates all the weights in the network. This gradient descent step is\n",
    "iterated (often thousands of times, using the same training examples multiple times) until the network performs acceptably well.\n",
    "\n",
    "<br>\n",
    "<a id='forward_propagation'>\n",
    "    <img src=\"images/multi_layer_perceptron.png\" alt=\"forward propagation\" width=\"45%\" height=\"45%\">\n",
    "</a>\n",
    "\n",
    "<br>\n",
    "The weight-update loop in Back Propagation may be iterated thousands of times in a typical application, and a variety of termination conditions can be used to halt the procedure. One may choose to halt after a <b>fixed number of iterations</b>\n",
    "through the loop, or <b>once the error on the training examples falls below some threshold</b> (a poor choice considering the tendency of ann to overfit the training examples), <b>or once the error on a separate validation set of examples meets some criterion</b>. The choice of termination criterion is an important one, because too few iterations can fail to reduce error sufficiently, and too many can lead to overfitting the training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "<br>\n",
    "Examining the algorithm we can see why it's called Back Propagation : we compute the error vector $ \\delta $ backward, starting from the final layer. If we think about the derivation, it is easy to see that the backward movement is simply a consequence of the fact that the cost is a function of outputs from the network. To understand how the cost varies with earlier weights and biases we need to repeatedly apply the chain rule, working backward through the layers to obtain usable expressions.\n",
    "\n",
    "<br>\n",
    "Artificial Neural Networks provide a <b>practical method for learning real-valued and vector-valued functions over continuous and discrete-valued attributes, in a way that is robust to noise in the training data</b>. The Back Propagation algorithm is the most common network learning method and has been successfully applied to a variety of learning tasks.\n",
    "\n",
    "<br>\n",
    "<b>Speed</b>\n",
    "\n",
    "<br>\n",
    "What makes <b>Back Propagation</b> clever is that it <b>enables us to simultaneously compute all the partial derivatives of the cost function using just one forward pass through the network, followed by one backward pass through the network</b>. Since the dominant computational cost in the forward pass is multiplying by the weight matrices, while in the backward pass it's multiplying by the transposes of the weight matrices, it's easy to see that these two operations have similar computational costs. It is then plausible to say that, roughly speaking, the total cost of backpropagation is comparable to making just two forward passes through the network. It's for this reason that Back Propagation is often called a \"fast algorithm\".\n",
    "\n",
    "<br>\n",
    "<b>Hypothesis Space</b>\n",
    "\n",
    "<br>\n",
    "The hypothesis space considered by the Back Propagation algorithm is the space of all functions that can be represented by assigning weights to the given, fixed network of interconnected units. <b>Feedforward networks containing three layers of units are able to approximate any function to arbitrary accuracy, given a sufficient (potentially very large) number of units in each\n",
    "layer</b>. Even networks of practical size are capable of representing a rich space of highly nonlinear functions, making feedforward networks a good choice for learning discrete and continuous functions whose general form is unknown in advance.\n",
    "\n",
    "<br>\n",
    "<b>Representational Power</b>\n",
    "\n",
    "<br>\n",
    "One of the most intriguing properties of Back Propagation is its <b>ability to invent new features that are not explicit in the input to the network</b>. In particular, the internal (hidden) layers of multilayer networks learn to represent intermediate features that are useful for learning the target function and that are only implicit in the network inputs.\n",
    "\n",
    "<br>\n",
    "<b>Overfitting</b>\n",
    "\n",
    "<br>\n",
    "Overfitting the training data is an important issue in ANN; overfitting results in networks that generalize poorly to new data despite excellent performance over the training data. <b>Cross-validation methods can be used to estimate an appropriate stopping point for gradient descent search</b> and thus to minimize the risk of overfitting.\n",
    "\n",
    "<br>\n",
    "A large number of different methods have been developed to avoid overfitting, which will be discussed in a different notebook :\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li> weight decay                    </li><br>\n",
    "    <li> weight sharing                  </li><br>\n",
    "    <li> weight sharing                  </li><br>\n",
    "    <li> early stopping                  </li><br>\n",
    "    <li> model averaging                 </li><br>\n",
    "    <li> bayesian fitting of neural nets </li><br>\n",
    "    <li> dropout                         </li><br>\n",
    "    <li> generative pre-training         </li>\n",
    "</ul>\n",
    "    \n",
    "Another way around overfitting is to decrease each weight by a small weight decay factor during each epoch. Learned networks with large (positive or negative) weights tend to have overfitted the data, because larger weights are needed to accommodate outliers in the data. Hence, keeping the weights low with a weight decay factor may help to steer the network from overfitting.\n",
    "\n",
    "<br>\n",
    "<b>Extensions</b>\n",
    "\n",
    "<br>\n",
    "Although Back Propagation is the most common ANN learning algorithm, many others have been proposed, including algorithms for more specialized tasks. For example, <b>recurrent neural network</b> methods train networks containing directed cycles, and algorithms such as <b>Cascade Correlation</b> alter the network structure as well as the network weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "         Tom Mitchell - \n",
    "         <a href=\"https://bit.ly/2IYw6Yc\">\n",
    "         Machine Learning</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Wikipedia - \n",
    "         <a href=\"https://bit.ly/2jMEOBe\">\n",
    "         Back Propagation</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Stanford CS class CS231n - \n",
    "         <a href=\"https://bit.ly/2Hggyyv\">\n",
    "         Convolutional Neural Networks for Visual Recognition</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Isaac Changhau - \n",
    "         <a href=\"https://bit.ly/2xsxKQC\">\n",
    "         Weight Initialization in Artificial Neural Networks</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Andy's Blog - \n",
    "         <a href=\"https://bit.ly/2vTlmaJ\">\n",
    "         An Explanation of Xavier Initialization</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Intoli - \n",
    "         <a href=\"https://bit.ly/2HumckF\">\n",
    "         Understanding Neural Network Weight Initialization</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Sewak et al. - \n",
    "         <a href=\"https://bit.ly/2Jis5RV\">\n",
    "         Practical Convolutional Neural Networks :  Implement advanced deep learning using Python</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Sebastian Raschka - \n",
    "         <a href=\"https://bit.ly/2IRtDis\">\n",
    "         What is the difference between a Perceptron, Adaline, and neural network model?</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Medium - Becoming Human - \n",
    "         <a href=\"https://bit.ly/2snSe7t\">\n",
    "         From Perceptron to Deep Neural Nets</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Swarthmore College - CS81 Adaptive Robotics - Lisa Meeden - \n",
    "         <a href=\"https://bit.ly/2IXpbCy\">\n",
    "         Derivation of Backpropagation</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Neural Networks and Deep Learning - \n",
    "         <a href=\"https://bit.ly/1m51jZ0\">\n",
    "         Chapter 2 - How the Back Propagation Algorithm Works</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         Imperial College London - Course V231 Artificial Intelligence - Simon Colton - \n",
    "         <a href=\"https://bit.ly/2xqduz9\">\n",
    "         Multi-Layer Artificial Neural Networks</a>        \n",
    "    </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
