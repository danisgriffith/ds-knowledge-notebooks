{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VIOLATION OF INDEPENDENCE OF THE ERROR TERMS\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "The assumption of independence of the disturbance terms (<b>A4</b>) states that : \n",
    "\n",
    "<br>\n",
    "<blockquote>\n",
    "The disturbance terms have zero conditional covariance across observations of the regressors\n",
    "</blockquote>\n",
    "\n",
    "<br>\n",
    "<blockquote>\n",
    "$\n",
    "    \\mathrm{Cov} \n",
    "    (\\boldsymbol{\\varepsilon_i},\\boldsymbol{\\varepsilon_s} \\mid \\boldsymbol{\\mathbf{X}_i},\\boldsymbol{\\mathbf{X}_s}) \n",
    "    \\ = \\ 0 \\quad \\forall i \\neq s\n",
    "$\n",
    "</blockquote>\n",
    "\n",
    "<br>\n",
    "When this assumption is not justified, the dependency usually appears because of a temporal component. Error terms correlated over time are said to be autocorrelated or serially correlated. \n",
    "\n",
    "<br>\n",
    "The current notebook will often refer to the violation of homoscedasticity; since <b>A3</b> and <b>A4</b> are the two assumptions that affect the covariance matrix of the disturbance terms, they share common features in the impact. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consequences\n",
    "\n",
    "<br>\n",
    "Just like homoscedasticity, independence of the error terms is not required for the OLS estimates to be <b>unbiased, consistent, and asymptotically normal</b>. \n",
    "\n",
    "<br>\n",
    "Violations of <b>A4</b> will affect our estimation however :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        the OLS estimator $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ will no longer have the lowest variance in the class of\n",
    "        the linear unbiased estimators (loss of efficiency)\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        as a direct consequence, OLS is no longer the Best Linear Unbiased Estimator of the population parameters\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the OLS estimator $\\mathrm{V}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}) $ ,and thus the standard errors as well, may be\n",
    "        both biased and inconsistent\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        biased standard errors lead to biased inference, meaning confidence intervals and the results of hypothesis tests are no\n",
    "        longer reliable        \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "It's important to notice that the consequences listed above are exactly the same arising from violations of homoscedasticity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact on test statistics\n",
    "\n",
    "<br>\n",
    "The F-statistic to test for overall significance of the regression may be inflated under positive serial correlation because the mean squared error (MSE) will tend to underestimate the population error variance. \n",
    "\n",
    "<br>\n",
    "Furthermore, positive serial correlation typically causes the ordinary least squares (OLS) standard errors for the regression coefficients to underestimate the true standard errors. As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small standard errors for the regression coefficient. These small standard errors will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the standard errors were correctly estimated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Models\n",
    "\n",
    "<br>\n",
    "A time series is a sequence of measurements of the same variable(s) made over time. Usually the measurements are made at evenly spaced times. To emphasize that we have measured values over time, we use the subscript $t$ instead of the usual $i$.\n",
    "\n",
    "<br>\n",
    "We have an autoregressive model when values from a time series are regressed on previous values from that same time series; in the regression model described by the equation below (which is just an example), the value of the response variable at the previous time period has become the main regressor : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{\\mathbf{Y}_{t}} \n",
    "    \\ = \\ \\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1} \\ \\boldsymbol{\\mathbf{Y}_{t-1}} + \\boldsymbol{\\epsilon_{t}}\n",
    "$\n",
    "\n",
    "<br>\n",
    "The order of the autoregression is the number of immediately preceding values in the series that are used to predict the value at the present time. With this definition in mind, we can say that the model in the example is a first-order autoregression, written as <b><i>AR(1)</i></b>.\n",
    "\n",
    "<br>\n",
    "More generally, a $k^\\text{th}$ order autoregression, <b><i>AR(k)</i></b>, is a multiple linear regression in which the value of the series at any time $t$ is a (linear) function of the values at times $ \\ t-1, t-2, \\dots, t-k \\ $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation and Partial Autocorrelation\n",
    "\n",
    "<br>\n",
    "The coefficient of correlation between two values in a time series is called the <b>autocorrelation function (ACF)</b>, and is a way to measure the linear relationship between an observation at time $t$ and the observations at previous times :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    ACF \\ = \\ \\mathrm{Corr}(\\boldsymbol{\\mathbf{Y}_t}, \\boldsymbol{\\mathbf{Y}_{t-k}})\n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "    [\\textbf{E1}] \n",
    "$\n",
    "\n",
    "<br>\n",
    "Here, the value of $k$ represents the time gap being considered and is called <b>lag</b>. In general, a lag-$k$ autocorrelation is the coefficient of correlation between values that are $k$ time periods apart. The ACF \n",
    "\n",
    "<br>\n",
    "If we assume an <i>AR(k)</i> model, then we may wish to only measure the association between $\\boldsymbol{\\mathbf{Y}_t}$ and $\\boldsymbol{\\mathbf{Y}_{t-k}}$ and filter out the linear influence of the random variables that lie in between (i.e. $ \\ \\boldsymbol{\\mathbf{Y}_{t-1}}, \\boldsymbol{\\mathbf{Y}_{t-2}}, \\dots, \\boldsymbol{\\mathbf{Y}_{t-(k-1)}} \\ $, which requires a transformation on the time series. Then by calculating the correlation of the transformed time series we obtain the <b>partial autocorrelation function (PACF)</b>.\n",
    "\n",
    "<br>\n",
    "The PACF is most useful for identifying the order of an autoregressive model. Specifically, sample partial autocorrelations that are significantly different from 0 indicate lagged terms of $\\mathbf{Y}$ that are useful predictors of $\\boldsymbol{\\mathbf{Y}_t}$. To help differentiate between ACF and PACF, think of them as analogues to R2 and partial R2 values as discussed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the lag\n",
    "\n",
    "<br>\n",
    "Graphical approaches to assessing the lag of an autoregressive model include looking at the ACF and PACF values versus the lag. \n",
    "\n",
    "<br>\n",
    "In a plot of ACF versus the lag, if you see large ACF values and a non-random pattern, then likely the values are serially correlated. In a plot of PACF versus the lag, the pattern will usually appear random, but large PACF values at a given lag indicate this value as a possible choice for the order of an autoregressive model. \n",
    "\n",
    "<br>\n",
    "It is important that the choice of the order makes sense. For example, suppose you have blood pressure readings for every day over the past two years. You may find that an <i>AR(1)</i> or <i>AR(2)</i> model is appropriate for modeling blood pressure. However, the PACF may indicate a large partial autocorrelation value at a lag of 17, but such a large order for an autoregressive model likely does not make much sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with Autoregressive Errors\n",
    "\n",
    "<br>\n",
    "The difficulty that often arises in the context of autoregressive models is that the disturbance terms may be correlated with each other. In other words, we have autocorrelation (or dependency) between the errors.\n",
    "\n",
    "<br>\n",
    "We may consider situations in which the error at one specific time is linearly related to the error at the previous time. That is, the errors themselves follow a simple linear regression model that can be written as :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{\\varepsilon_{t}} \\ = \\ \\boldsymbol{\\rho} \\ \\boldsymbol{\\varepsilon_{t-1}} + \\boldsymbol{\\omega_{t}}\n",
    "    \\qquad \\qquad \\text{where } \\mid \\ \\boldsymbol{\\rho} \\mid \\ < 1\n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\\n",
    "    [\\textbf{E2}] \n",
    "$\n",
    "\n",
    "<br>\n",
    "Here, $\\boldsymbol{\\rho}$ is called the <b>autocorrelation parameter</b> and $ \\ \\boldsymbol{\\omega_{t}} \\ $ is a new error term that follows the usual assumptions we make about disturbance terms : $ \\ \\boldsymbol{\\omega_{t}} \\approx IID \\ \\textit{N}(0,\\boldsymbol{\\sigma^2}) \\ $. This model says that the error at time $t$ can be predicted from a fraction of the error at the previous time period plus some new random perturbation.\n",
    "\n",
    "<br>\n",
    "We can use partial autocorrelation function (PACF) plots to help us assess appropriate lags for the errors in a regression model with autoregressive errors. Specifically, we first fit a multiple linear regression model to our time series data and store the residuals. Then we can look at a plot of the PACF for the residuals versus the lag. Large sample partial autocorrelations that are significantly different from 0 indicate lagged terms of $\\boldsymbol{\\epsilon}$ that may be useful predictors of $\\boldsymbol{\\epsilon_{t}}$.\n",
    "\n",
    "<br>\n",
    "There are several different methods for estimating the regression parameters when we have errors with an autoregressive structure and we will introduce a few of these methods later in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "<br>\n",
    "The easiest way to assess the presence of dependency is by producing a scatterplot of the residuals versus the time measurement for that observation, assuming the data are arranged according to a time sequence order. If the data are independent, then the residuals should look randomly scattered about zero; however, if a noticeable pattern emerges (particularly one that is cyclical) then dependency is likely an issue.\n",
    "\n",
    "<br>\n",
    "If we suspect first-order autocorrelation with the errors, then a formal test does exist regarding the autocorrelation parameter $\\boldsymbol{\\rho}$.\n",
    "\n",
    "<br>\n",
    "In statistics, the <b>Durbin – Watson statistic</b> is a test statistic used to detect the presence of autocorrelation in the residuals from a regression analysis. Durbin and Watson applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, the Durbin - Watson test is constructed as :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align} \n",
    "        H_{0} \\ &: \\ \\boldsymbol{\\rho} = 0 \n",
    "        \\newline\n",
    "        H_{A} \\ &: \\ \\boldsymbol{\\rho} \\neq 0\n",
    "    \\end{align}    \n",
    "$\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{d} \\ = \\\n",
    "        \\dfrac \n",
    "            { \\sum _{t=2}^{m} (\\boldsymbol{e_{t}} - \\boldsymbol{e_{t-1}})^\\boldsymbol{2} } \n",
    "            { \\sum _{t=1}^{m} \\boldsymbol{e_{t}^2} } \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\\n",
    "    [\\textbf{E3}] \n",
    "$\n",
    "\n",
    "<br>\n",
    "The null hypothesis of $ H_{0} $ means that $ \\ \\boldsymbol{e_{t}} = \\boldsymbol{\\omega_{t}} \\ $; in other words, the error term in one period is not correlated with the error term in the previous period. The alternative hypothesis $ H_{A} $ means the error term in one period is either positively or negatively correlated with the error term in the previous period.\n",
    "\n",
    "<br>\n",
    "The value of $\\boldsymbol{d}$ always lies between 0 and 4; since $\\boldsymbol{d}$ is approximately equal to $ \\ 2(1 − r) \\ $ (where $r$ is the sample autocorrelation of the residuals), $ \\boldsymbol{d} = 2 \\ $ indicates no autocorrelation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Durbin – Watson statistic is substantially less than 2, there is evidence of positive serial correlation. As a rough rule of thumb, if the test statistic is less than 1, there may be cause for alarm. Small values of $\\boldsymbol{d}$ indicate successive error terms are, on average, close in value to one another, or positively correlated. If $\\boldsymbol{d} > 2$, successive error terms are, on average, much different in value from one another, i.e., negatively correlated.\n",
    "\n",
    "<br>\n",
    "<b>Positive serial correlation</b> is serial correlation in which a positive error for one observation increases the chances of a positive error for another observation. To test for positive autocorrelation at significance $\\mathbf{\\alpha}$, the test statistic $\\boldsymbol{d}$ is compared to lower and upper critical values $\\boldsymbol{d_L(\\alpha)}$ and $\\boldsymbol{d_U(\\alpha)}$ :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        if $ \\ \\boldsymbol{d} < \\boldsymbol{d_L(\\alpha)} \\ $, there is statistical evidence that the error terms are positively\n",
    "        autocorrelated\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        if $ \\ \\boldsymbol{d} > \\boldsymbol{d_U(\\alpha)} \\ $, there is no statistical evidence that the error terms are\n",
    "        positively autocorrelated\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        if $ \\ \\boldsymbol{d_L(\\alpha)} < \\boldsymbol{d} < \\boldsymbol{d_U(\\alpha)} \\ $, the test is inconclusive      \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "<b>Negative serial correlation</b> implies that a positive error for one observation increases the chance of a negative error for another observation, and viceversa. To test for negative autocorrelation at significance $\\mathbf{\\alpha}$, the test statistic $(4 − \\boldsymbol{d})$ is compared to lower and upper critical valuesis $\\boldsymbol{d_L(\\alpha)}$ and $\\boldsymbol{d_U(\\alpha)}$ :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        if $ \\ (4 − \\boldsymbol{d}) < \\boldsymbol{d_L(\\alpha)} \\ $, there is statistical evidence that the error terms are\n",
    "        negatively autocorrelated\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        if $ \\ (4 − \\boldsymbol{d}) > \\boldsymbol{d_U(\\alpha)} \\ $, there is no statistical evidence that the error terms are\n",
    "        negatively autocorrelated\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        if $ \\ \\boldsymbol{d_L(\\alpha)} < (4 − \\boldsymbol{d}) < \\boldsymbol{d_U(\\alpha)} \\ $, the test is inconclusive   \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "The critical values $\\boldsymbol{d_L(\\alpha)}$ and $\\boldsymbol{d_U(\\alpha)}$ vary by level of significance ($\\alpha$), the number of observations, and the number of predictors in the regression equation. Their derivation is complex, and statisticians typically obtain these two values from statistical texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "<br>\n",
    "If the Durbin – Watson statistic indicates the presence of serial correlation of the residuals (and consequently of the error terms), one of the first remedial measures should be to investigate the omission of a key predictor variable. \n",
    "\n",
    "<br>\n",
    "If such a predictor does not aid in reducing or eliminating the autocorrelation, then certain transformations on the variables can be performed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cochrane – Orcutt Estimation\n",
    "\n",
    "<br>Cochrane–Orcutt estimation is a procedure which adjusts a linear model for serial correlation in the error term. Consider the model : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{\\mathbf{Y}_t} \\ = \\\n",
    "        \\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1} \\ \\boldsymbol{\\mathbf{X}_t} + \\boldsymbol{\\varepsilon_t}\n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad  \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "    [\\textbf{E4}] \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "If the process generating the residuals is found to be a stationary first-order autoregressive structure ($ \\boldsymbol{\\varepsilon_{t}} = \\boldsymbol{\\rho} \\ \\boldsymbol{\\varepsilon_{t-1}} + \\boldsymbol{\\omega_{t}} $ with the errors $\\boldsymbol{\\omega_{t}}$ being white noise), then the Cochrane – Orcutt procedure transforms the model by taking a quasi-difference :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\boldsymbol{\\mathbf{Y}_t} - \\boldsymbol{\\rho} \\ \\boldsymbol{\\mathbf{Y}_{t-1}} \n",
    "        &=\n",
    "        \\newline\n",
    "        &= \n",
    "            ( \n",
    "                      \\boldsymbol{\\beta_0}\n",
    "                    + \\boldsymbol{\\beta_1} \\ \\boldsymbol{\\mathbf{X}_t} \n",
    "                    + \\boldsymbol{\\varepsilon_t} \n",
    "            ) \n",
    "            - \\boldsymbol{\\rho} (\n",
    "                      \\boldsymbol{\\beta_0} \n",
    "                    + \\boldsymbol{\\beta_1} \\ \\boldsymbol{\\mathbf{X}_{t-1}} \n",
    "                    + \\boldsymbol{\\varepsilon_{t-1}}\n",
    "            )\n",
    "            & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\ \\text{since } \\textbf{E2}\n",
    "        \\newline\n",
    "        &= \n",
    "              \\boldsymbol{\\beta_0} (1 - \\boldsymbol{\\rho})\n",
    "            + \\boldsymbol{\\beta_1} (\\boldsymbol{\\mathbf{X}_t} - \\boldsymbol{\\rho} \\ \\boldsymbol{\\mathbf{X}_{t-1}})\n",
    "            + \\boldsymbol{\\omega_{t}}\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{ {\\mathbf{Y}_t}^* } =\n",
    "          \\boldsymbol{ {\\beta_0}^* } \n",
    "        + \\boldsymbol{ {\\beta_1}^* } \\ \\boldsymbol{ {\\mathbf{X}_t}^* } \n",
    "        + \\boldsymbol{ {\\varepsilon_t}^* }  \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "    \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad  \\qquad \\qquad \\qquad\n",
    "    [\\textbf{E5}] \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The procedure in can be described as an iterative process, consisting of the following steps :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        estimate the autocorrelation parameter $\\boldsymbol{\\rho}$ : if $\\boldsymbol{\\rho}$ is not known, then it can be\n",
    "        estimated by first regressing the original model and obtaining the residuals $\\boldsymbol{e}$, then regressing\n",
    "        $\\boldsymbol{e_{t}}$ on $\\boldsymbol{e_{t-1}}$ to compute an estimate of the autocorrelation parameter which is often\n",
    "        called $\\boldsymbol{r}$\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        transform the original model <b>E4</b> into <b>E5</b> \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        regress $\\boldsymbol{ {\\mathbf{Y}_t}^* }$ on the transformed predictors using ordinary least squares to obtain estimates\n",
    "        of the transformed parameters $ \\boldsymbol{ {\\beta_0}^* }, \\dots, \\boldsymbol{ {\\beta_{p-1}}^* }$\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        examine the current regression residuals and determine if autocorrelation is still present (using the Durbin - Watson\n",
    "        test for example). If autocorrelation is still present, then iterate this procedure.\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        if autocorrelation appears to be corrected, then we can transform the estimated parameters back to their original scale\n",
    "        by setting <br><br>\n",
    "        $ \n",
    "            \\quad\n",
    "            \\begin{align}\n",
    "                \\boldsymbol{\\hat{\\beta_0}} &= \\boldsymbol{\\hat{\\beta_0}^*} / \\ (1 - r)\n",
    "                \\newline\n",
    "                \\boldsymbol{\\hat{\\beta_j}} &= \n",
    "                    \\boldsymbol{\\hat{\\beta_j}^*} \\quad \\forall j \\quad \\text{(j = 1,} \\dots \\text{, p - 1)}\n",
    "            \\end{align}\n",
    "        $ <br><br>       \n",
    "        Notice that only the intercept parameter requires a transformation. Furthermore, the standard errors of the regression\n",
    "        estimates for the original scale can also be obtained by setting <br><br>\n",
    "        $\n",
    "            \\quad\n",
    "            \\begin{align}\n",
    "                SE(\\boldsymbol{\\hat{\\beta_0}}) &= \\ SE(\\boldsymbol{\\hat{\\beta_0}}) \\ / \\ (1 - r) \n",
    "                \\newline\n",
    "                SE(\\boldsymbol{\\hat{\\beta_j}}) &= \\\n",
    "                    SE(\\boldsymbol{\\hat{\\beta_j}}) \\quad \\forall j \\quad \\text{(j = 1,} \\dots \\text{, p - 1)}\n",
    "            \\end{align}\n",
    "        $\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "One thing to note about the Cochrane - Orcutt approach is that it does not always work properly; this occurs primarily because if the errors are positively autocorrelated, then the estimated autocorrelation coefficient $\\boldsymbol{r}$ tends to underestimate $\\boldsymbol{\\rho}$. When this bias is serious, it can seriously reduce the effectiveness of the Cochrane - Orcutt procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        PennState University -  Stat 501 - \n",
    "        <a href=\"https://bit.ly/2kozxN2\">\n",
    "        Lesson 14 : Time Series and Autocorrelation</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Wikipedia - \n",
    "        <a href=\"https://bit.ly/2sa3G6B\">\n",
    "        Cochrane Orcutt estimation</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Wikipedia - \n",
    "        <a href=\"https://bit.ly/2xEofZR\">\n",
    "        Durbin Watson statistic</a>\n",
    "    </li>    \n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
