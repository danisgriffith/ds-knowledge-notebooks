{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GAUSS-MARKOV THEOREM\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "The reason OLS estimation is so popular is that, under the appropriate assumptions, the OLS coefficient estimators have several desirable statistical properties. In this notebook we will examine some of these statistical properties, primarily (but not only) in terms of the OLS slope coefficient estimator $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ .\n",
    "\n",
    "<br>\n",
    "Let's start by bringing up the two equations we defined for linear regression, and the OLS estimators : <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "$\n",
    "    \\begin{align}\n",
    "        \\boldsymbol{\\mathbf{Y}_i} = \\boldsymbol{\\beta}\\boldsymbol{\\mathbf{X}_i} + \\boldsymbol{\\varepsilon_i}\n",
    "        & \\quad \\boldsymbol{\\text{PRE}}\n",
    "        \\newline\n",
    "        \\boldsymbol{\\hat{\\mathbf{Y}}}\\boldsymbol{_i}\n",
    "        = \\boldsymbol{\\hat{\\beta}}\\boldsymbol{\\mathbf{X}_i} + \\boldsymbol{\\mathbf{e}_i}\n",
    "        & \\quad \\boldsymbol{\\text{SRE}}\n",
    "    \\end{align}\n",
    "$\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "$\n",
    "    \\begin{align}\n",
    "        \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "        &= \n",
    "            \\dfrac\n",
    "                { \n",
    "                    \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{X}_i} \\boldsymbol{\\mathbf{Y}_i} \n",
    "                    - m \\ \\overline{\\mathbf{X}} \\overline{\\mathbf{Y}} \n",
    "                }\n",
    "                { \\sum_{i=1}^{m} \\boldsymbol{{\\mathbf{X}_i}^2} - m \\ \\boldsymbol{\\overline{\\mathbf{X}}^2} }\n",
    "        \\newline\n",
    "        &= \n",
    "            \\dfrac\n",
    "                { \n",
    "                    \\sum_{i=1}^{m}\n",
    "                    \\big( \\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}} \\big)\n",
    "                    \\big( \\boldsymbol{\\mathbf{Y}_i} - \\overline{\\mathbf{Y}} \\big)\n",
    "                }\n",
    "                {\n",
    "                    \\sum_{i=1}^{m} \n",
    "                    \\big( \\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}} \\big)^2\n",
    "                }\n",
    "        = \n",
    "            \\dfrac\n",
    "                {\\mathrm{Cov}(\\boldsymbol{\\mathbf{X}_i}, \\boldsymbol{\\mathbf{Y}_i})}\n",
    "                {\\mathrm{Var}(\\boldsymbol{\\mathbf{X}_i})} \n",
    "        \\newline\n",
    "        &= \n",
    "            \\sum_{i=1}^{m} \n",
    "            \\boldsymbol{\\mathbf{x}_i} \\boldsymbol{\\mathbf{y}_i}               \n",
    "    \\end{align}\n",
    "$\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "$\n",
    "    \\quad\n",
    "    \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0} \n",
    "    = \\overline{\\mathbf{Y}} - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\overline{\\mathbf{X}}\n",
    "$\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gauss–Markov theorem\n",
    "\n",
    "<br>\n",
    "In statistics, the Gauss–Markov theorem states that, under the assumptions [<b>A1 - A8</b>] of the Classical Linear Regression Model, the <b>B</b>est <b>L</b>inear <b>U</b>nbiased <b>E</b>stimator (provided it exists, <b>BLUE</b>) of the regression coefficients is given by the ordinary least squares (<b>OLS</b>) estimator.\n",
    "\n",
    "<br>\n",
    "Equivalently, the theorem establishes that under the <b>CLRM</b> assumptions, the OLS coefficient estimators $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-j}$ (j = 0, 1) are the minimum-variance estimators in the class of all linear\n",
    "unbiased estimators of the corresponding population parameters.\n",
    "\n",
    "<br>\n",
    "Although all the <b>CLRM</b> assumptions are actually needed in the broader context of linear regression, only a few of them are usually cited in the Gauss-Markov theorem, in particular only those concerned with the disturbance term $\\boldsymbol{\\varepsilon}$ : \n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>linearity (A1)</b>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>strict exogeneity (A2)</b>\n",
    "    </li>  \n",
    "    <li>\n",
    "        <b>spherical errors (A3 + A4)</b>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>full rank (A6 + A8)</b>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "We will now proceed in the demonstration of the thereom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GM1] Proof of Linearity\n",
    "\n",
    "<br>\n",
    "A linear estimator of $\\boldsymbol{\\beta_j}$ is a linear combination\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{j} \n",
    "    = \\boldsymbol{\\mathbf{c}_{1j}} \\boldsymbol{\\mathbf{Y}_1} + \\cdots + \\boldsymbol{\\mathbf{c}_{mj}} \\boldsymbol{\\mathbf{Y}_m}\n",
    "$\n",
    "\n",
    "<br>\n",
    "in which the coefficients $\\boldsymbol{\\mathbf{c}_{ij}}$ are not allowed to depend on the underlying coefficients $\\boldsymbol{\\beta_j}$, since those are not observable, but are allowed to depend on the values $\\boldsymbol{\\mathbf{X}_{ij}}$, since these data are observable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's now re-write the formula for $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ :\n",
    "\n",
    "<br>\n",
    "$ \n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} &=\n",
    "            \\frac\n",
    "                {\\sum_{i=1}^{m} \n",
    "                    (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})\n",
    "                    (\\boldsymbol{\\mathbf{Y}_i} - \\overline{\\mathbf{Y}})\n",
    "                }\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            = \\frac\n",
    "                {\n",
    "                      \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \\boldsymbol{\\mathbf{Y}_i}\n",
    "                    - \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \\overline{\\mathbf{Y}}\n",
    "                }\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            = \\frac\n",
    "                {\n",
    "                      \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \\boldsymbol{\\mathbf{Y}_i}\n",
    "                    - \\overline{\\mathbf{Y}} \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})\n",
    "                }\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\newline\n",
    "            & = \\frac\n",
    "                {\n",
    "                      \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \\boldsymbol{\\mathbf{Y}_i}\n",
    "                    - \\overline{\\mathbf{Y}} 0\n",
    "                }\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            = \\frac\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \\boldsymbol{\\mathbf{Y}_i} }\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            = \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{Y}_i}            \n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "where the $\\boldsymbol{\\mathbf{c}_i}$ are defined by\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{\\mathbf{c}_i} = \\dfrac\n",
    "        {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})}\n",
    "        {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "and have the following properties :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        $\n",
    "            \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \n",
    "            \\ = \\ \\dfrac\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})}\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\ = \\ \\dfrac\n",
    "                {1}{\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "                \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})\n",
    "            \\ = \\ 0\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "            [\\textbf{P1}] \n",
    "        $ \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $\n",
    "            \\sum_{i=1}^{m} {\\boldsymbol{\\mathbf{c}_i}}^2 \n",
    "            \\ = \\ \\dfrac\n",
    "                { \\big[ \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \\big] ^2 }\n",
    "                { \\big[ \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2 \\big] ^2 }\n",
    "            \\ = \\ \\dfrac\n",
    "                {1}\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "            [\\textbf{P2}] \n",
    "        $ \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $\n",
    "            \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{X}_i}\n",
    "            \\ = \\  \n",
    "                  \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{X}_i}\n",
    "                - \\overline{\\mathbf{X}} \\sum_{i=1}^{N} \\boldsymbol{\\mathbf{c}_i}\n",
    "            \\ = \\  \n",
    "                  \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{X}_i}\n",
    "                - \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\overline{\\mathbf{X}} \n",
    "            \\ = \\  \\sum_{i=1}^{N} \\boldsymbol{\\mathbf{c}_i} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})\n",
    "            \\ = \\  \\dfrac\n",
    "                {\n",
    "                    \\sum_{i=1}^{m} \n",
    "                    (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) \n",
    "                    (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})\n",
    "                }\n",
    "                {\\sum_{i=1}^{N} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\ = \\  1\n",
    "            \\qquad \\qquad \\quad\n",
    "            [\\textbf{P3}] \n",
    "        $ \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now that we know the properties of the coefficients $\\boldsymbol{\\mathbf{c}_i}$, we will see that :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "        &= \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{Y}_i}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i}\n",
    "             (\\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1}\\boldsymbol{\\mathbf{X}_i} + \\boldsymbol{\\varepsilon_i})\n",
    "        \\newline\n",
    "        &= \n",
    "              \\boldsymbol{\\beta_0} \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i}\n",
    "            + \\boldsymbol{\\beta_1} \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{X}_i}\n",
    "            + \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\varepsilon_i}\n",
    "            & \\text{since } \\textbf{P1} \\text{ and } \\textbf{P3}\n",
    "        \\newline\n",
    "        &= \\boldsymbol{\\beta_1} + \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\varepsilon_i}\n",
    "        & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "        \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "        [\\textbf{E1}] \n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Therefore, the OLS estimator $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ (and by analogy  $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}$) is a linear estimator, specifically a linear function of the disturbance terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GM1] Proof of Linearity in matrix form\n",
    "\n",
    "<br>\n",
    "A quick look at the notebook regarding the OLS estimation will remind us of the following matrix form :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "        &= (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\\mathbf{Y} \n",
    "        \\newline\n",
    "        &= \\boldsymbol{\\beta_1} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Since we can write $ \\quad \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} = {\\boldsymbol{\\beta}_1} + \\mathbf{A}\\boldsymbol{\\varepsilon} \\quad $ where $\\mathbf{A} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}$, it is easy to see that $\\quad \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ is a linear function of the disturbance terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GM2] Proof of Unbiasedness\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GM2 | Unbiasedness of $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ ]\n",
    "\n",
    "<br>\n",
    "This proof follows directly by <b>E1</b> : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathbf{E} \\big[ \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\big] \n",
    "        &=\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            & \\text{since } \\textbf{E1}\n",
    "        \\newline \n",
    "        &= \n",
    "              \\mathbf{E} \\big[ \\boldsymbol{\\beta_1} \\big] \n",
    "            + \\mathbf{E} \\big[ \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\varepsilon_i} \\big] \n",
    "            & \\text{conditioning on } \\mathbf{X}\n",
    "        \\newline \n",
    "        &=\n",
    "              \\boldsymbol{\\beta_1} \n",
    "            + \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i} \n",
    "              \\mathbf{E} \\big[ \\boldsymbol{\\varepsilon_i} \\mid \\boldsymbol{\\mathbf{X}_i} \\big] \n",
    "              & \\text{strinct exogeneity (} \\textbf{A2} \\text{)}\n",
    "        \\newline             \n",
    "        &= \\boldsymbol{\\beta_1}\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Conditioning on the sample values of the regressor $\\mathbf{X}$ means that the coefficients $\\boldsymbol{\\mathbf{c}_i}$ are treated as non-random, since they are functions only of the sample values $\\boldsymbol{\\mathbf{X}_i}$. \n",
    "\n",
    "<br>\n",
    "Therefore, the OLS estimator $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ is an unbiased estimator of the corresponding population parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GM2 | Unbiasedness of $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}$ ]\n",
    "\n",
    "<br>\n",
    "This proof follows directly by the population regression equation (<b>PRE</b>) : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align*}\n",
    "        \\quad & \\quad\n",
    "            \\boldsymbol{\\mathbf{Y}_i} \n",
    "            = \\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1} \\boldsymbol{\\mathbf{X}_i} + \\boldsymbol{\\varepsilon_i}\n",
    "            & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "            \\text{averaging over the } m \\text{ observations}\n",
    "        \\newline\n",
    "        \\Rightarrow & \\quad\n",
    "            \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{Y}_i} \n",
    "            = \\frac{1}{m} \\ m \\ \\boldsymbol{\\beta_0}\n",
    "            + \\frac{\\boldsymbol{\\beta_1}} {m} \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{X}_i} \n",
    "            + \\frac{1}{m} \\sum_{i=1}^{m} \\boldsymbol{\\varepsilon_i}     \n",
    "        \\newline \n",
    "        \\Rightarrow & \\quad\n",
    "            \\overline{\\mathbf{Y}} \n",
    "            = \\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1} \\overline{\\mathbf{X}} + \\overline{\\boldsymbol{\\varepsilon}}  \n",
    "        \\newline \\newline\n",
    "        &\\quad\n",
    "             & \\text{by definition of the OLS estimator } \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "        \\newline\n",
    "        \\Rightarrow &\\quad\n",
    "            \\begin{aligned}[T]\n",
    "                \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "                &=   \\overline{\\mathbf{Y}}\n",
    "                  - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\overline{\\mathbf{X}}\n",
    "                \\newline\n",
    "                &=  (\\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1} \\overline{\\mathbf{X}} + \\overline{\\boldsymbol{\\varepsilon}})\n",
    "                  - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\overline{\\mathbf{X}}\n",
    "                \\newline\n",
    "                &=   \\boldsymbol{\\beta_0} \n",
    "                   + (\\boldsymbol{\\beta_1} - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \\overline{\\mathbf{X}} \n",
    "                   + \\overline{\\boldsymbol{\\varepsilon}}\n",
    "            \\end{aligned}\n",
    "        \\newline \\newline\n",
    "        &\\quad\n",
    "             & \\text{conditioning on } \\mathbf{X}\n",
    "        \\newline\n",
    "        &\\quad\n",
    "             & \\text{zero unconditional mean (} \\textbf{A2} \\text{)}\n",
    "        \\newline\n",
    "        \\Rightarrow &\\quad\n",
    "            \\begin{aligned}[T]\n",
    "                \\mathbf{E} \\big[ \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0} \\big]\n",
    "                &=     \\mathbf{E} \\big[ \\boldsymbol{\\beta_0} \\big]\n",
    "                    + \\mathbf{E} \n",
    "                      \\big[ \\ (\\boldsymbol{\\beta_1} - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \\overline{\\mathbf{X}} \\ \\big]\n",
    "                    + \\mathbf{E} \\big[ \\overline{\\boldsymbol{\\varepsilon}} \\big]\n",
    "                \\newline\n",
    "                &=                \n",
    "                      \\boldsymbol{\\beta_0}\n",
    "                    + \\overline{\\mathbf{X}} \\mathbf{E} \n",
    "                      \\big[ \\ \\boldsymbol{\\beta_1} - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\ \\big]\n",
    "                    + \\mathbf{E} \\big[ \\overline{\\boldsymbol{\\varepsilon}} \\big]\n",
    "                \\newline\n",
    "                &=\n",
    "                      \\boldsymbol{\\beta_0}\n",
    "                    + \\overline{\\mathbf{X}} \n",
    "                      \\big[ \\ \n",
    "                          \\mathbf{E} [\\boldsymbol{\\beta_1}] - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}] \n",
    "                       \\ \\big]\n",
    "                    + \\mathbf{E} \\big[ \\overline{\\boldsymbol{\\varepsilon}} \\big]\n",
    "                \\newline\n",
    "                &=\n",
    "                      \\boldsymbol{\\beta_0}\n",
    "                    + \\overline{\\mathbf{X}} \\big[ \\ \\boldsymbol{\\beta_1} - \\boldsymbol{\\beta_1} \\ \\big]\n",
    "                \\newline\n",
    "                &= \\boldsymbol{\\beta_0}\n",
    "            \\end{aligned}\n",
    "    \\end{align*}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Conditioning on the sample values of the regressor $\\mathbf{X}$ means that $\\overline{\\mathbf{X}}$ are treated as non-random in taking expectations, since it is a function only of the sample values $\\boldsymbol{\\mathbf{X}_i}$. \n",
    "\n",
    "<br>\n",
    "Therefore, the OLS estimator $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}$ is an unbiased estimator of the corresponding population parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GM2 | Unbiasedness of $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$ in matrix form ] \n",
    "\n",
    "<br>\n",
    "Again the same matrix form we used before to prove linearity in matrix form :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "        &= (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\\mathbf{Y} \n",
    "        \\newline\n",
    "        &= \\boldsymbol{\\beta_1} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "It is easy to show that, as long as $\\mathbf{X}$ is either non-stochastic or stochastic but independent of the disturbance terms, the OLS estimators $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-j}$ are unbiased estimators : \n",
    "\n",
    "<br>\n",
    "If $\\mathbf{X}$ is non-stochastic (fixed) :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathbf{E} \\big[ \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\big]\n",
    "        &= \n",
    "              \\boldsymbol{\\beta_1} \n",
    "            + \\mathbf{E} \\big[ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \\big]\n",
    "        \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\beta_1} \n",
    "            + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{E} \\big[ \\boldsymbol{\\varepsilon} \\big]\n",
    "            & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\text{strict exogeneity (} \\textbf{A2} \\text{)}\n",
    "        \\newline\n",
    "        &= \\boldsymbol{\\beta_1} \n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "If $\\mathbf{X}$ is stochastic but uncorrelated with the disturbance term :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathbf{E} \\big[ \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\big]\n",
    "        &= \n",
    "              \\boldsymbol{\\beta_1} \n",
    "            + \\mathbf{E} \\big[ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \\big]\n",
    "        \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\beta_1} \n",
    "            + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{E} \\big[ \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \\big]\n",
    "            & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\text{strict exogeneity (} \\textbf{A2} \\text{)}\n",
    "        \\newline\n",
    "        &= \\boldsymbol{\\beta_1} \n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix of the OLS estimators\n",
    "\n",
    "<br>\n",
    "In probability theory and statistics, a <b>covariance matrix</b> (also known as dispersion matrix or <b>variance–covariance matrix</b>) is a matrix whose element in the $ij$ position is the covariance between the $i^\\text{th}$ and $j^\\text{th}$ elements of a random vector. A random vector is a random variable with multiple dimensions.\n",
    "\n",
    "<br>\n",
    "Because the covariance of the $i^\\text{th}$ random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables. Because the covariance of the $i^\\text{th}$ random variable with the $j^\\text{th}$ one is the same thing as the covariance of the $j^\\text{th}$ random variable with the $i^\\text{th}$ one, every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. We will now determine the covariance matrix of the OLS estimators $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}$ :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{V}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}) \n",
    "        &=\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "            & \\text{when } \\star \\text{ is a vector random variable}            \n",
    "        \\newline\n",
    "        &\n",
    "            & \n",
    "            \\mathrm{Var}(\\star) = \\mathbf{E} \n",
    "            \\big[ \n",
    "                \\big(\\star - \\mathbf{E}[\\star]\\big)\n",
    "                \\big(\\star - \\mathbf{E}[\\star]\\big)^{\\top} \n",
    "            \\big]\n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \n",
    "            \\big[ \\\n",
    "                \\big(\n",
    "                          \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS} \n",
    "                        - \\mathbf{E}[\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}]\n",
    "                \\big)\n",
    "                \\big(\n",
    "                          \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS} \n",
    "                        - \\mathbf{E}[\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}]\n",
    "                \\big)^{\\top}\n",
    "            \\ \\big] \n",
    "            & \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS} \\text{ is an unbiased estimator of } \\boldsymbol{\\beta}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \n",
    "            \\big[ \\\n",
    "                \\big(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS} - \\boldsymbol{\\beta}\\big)\n",
    "                \\big(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS} - \\boldsymbol{\\beta}\\big)^{\\top}\n",
    "            \\ \\big] \n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \n",
    "            \\big[ \\ \n",
    "                \\big(\n",
    "                      \\boldsymbol{\\beta} \n",
    "                    + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \n",
    "                    - \\boldsymbol{\\beta_1}\n",
    "                \\big)\n",
    "                \\big(\n",
    "                      \\boldsymbol{\\beta} \n",
    "                    + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \n",
    "                    - \\boldsymbol{\\beta}\n",
    "                \\big)^{\\top}\n",
    "            \\ \\big] \n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \n",
    "            \\big[ \\\n",
    "                    \\big( (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \\big)\n",
    "                    \\big( (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \\big) ^{\\top}\n",
    "            \\ \\big]\n",
    "            & \n",
    "            \\big[ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\boldsymbol{\\varepsilon} \\big] ^{\\top}\n",
    "            = \\boldsymbol{\\varepsilon}^{\\top} \\ \\big[ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\big] ^{\\top}\n",
    "        \\newline\n",
    "        &\n",
    "            & \n",
    "            = \\boldsymbol{\\varepsilon}^{\\top} \\ \\mathbf{X} \\big[(\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\big] ^{\\top}\n",
    "            = \\boldsymbol{\\varepsilon}^{\\top} \\ \\mathbf{X} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \n",
    "            \\big[ \\\n",
    "                    (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \n",
    "                    \\ \\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top} \\\n",
    "                    \\mathbf{X} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\n",
    "            \\ \\big]\n",
    "            & \\text{conditioning on } \\mathbf{X}\n",
    "        \\newline\n",
    "        &= \n",
    "            (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\n",
    "            \\ \\mathbf{E} \\big[ \\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top} \\big] \\         \n",
    "            \\mathbf{X} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}         \n",
    "            & [\\textbf{E2}] \n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "$\\mathbf{E} \\big[ \\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top} \\big]$ is the covariance matrix of the disturbance term $\\boldsymbol{\\varepsilon}$. Despite being a $ \\text{m x m}$ matrix and thus a potentially large one, under the assumption of spherical errors (homoscedasticity and no autocorrelation of the error terms) the covariance matrix of the disturbance terms simplifies greatly : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "    \\mathbf{E} [ \\varepsilon \\varepsilon^{\\top} \\mid X ]\n",
    "    \\quad &= \\quad\n",
    "    \\mathbf{E}\n",
    "    \\begin{bmatrix}\n",
    "        \\varepsilon_1 \\mid X \\\\\n",
    "        \\varepsilon_2 \\mid X \\\\\n",
    "        \\vdots               \\\\\n",
    "        \\vdots               \\\\\n",
    "        \\varepsilon_m \\mid X\n",
    "    \\end{bmatrix}_\\textit{ m x 1 }\n",
    "    \\begin{bmatrix}\n",
    "        \\varepsilon_1 \\mid X &\n",
    "        \\varepsilon_2 \\mid X &\n",
    "        \\dots                &\n",
    "        \\dots                &\n",
    "        \\varepsilon_m \\mid X \n",
    "    \\end{bmatrix}_\\textit{ 1 x m }\n",
    "    \\newline \\newline\n",
    "    &= \\quad\n",
    "    \\mathbf{E}\n",
    "    \\begin{bmatrix}\n",
    "        {\\varepsilon_1}^2 \\mid X           &  \\varepsilon_1\\varepsilon_2 \\mid X & \\dots  & \\varepsilon_1\\varepsilon_m \\mid X  \\\\\n",
    "        \\varepsilon_2\\varepsilon_1 \\mid X  &  {\\varepsilon_2}^2 \\mid X          & \\dots  & \\varepsilon_2\\varepsilon_m \\mid X  \\\\\n",
    "        \\vdots                             &  \\vdots                            & \\vdots & \\vdots                             \\\\\n",
    "        \\vdots                             &  \\vdots                            & \\ddots & \\vdots                             \\\\\n",
    "        \\varepsilon_m\\varepsilon_1 \\mid X  &  \\varepsilon_m\\varepsilon_2 \\mid X & \\dots  & {\\varepsilon_m}^2 \\mid X  \n",
    "    \\end{bmatrix}_\\textit{ m x m }\n",
    "    \\quad = \\quad \n",
    "    \\begin{bmatrix}\n",
    "          \\mathbf{E} [ {\\varepsilon_1}^2 \\mid X ]\n",
    "        & \\mathbf{E} [ \\varepsilon_1\\varepsilon_2 \\mid X ]\n",
    "        & \\dots  \n",
    "        & \\mathbf{E} [ \\varepsilon_1\\varepsilon_m \\mid X ] \n",
    "        \\\\\n",
    "          \\mathbf{E} [ \\varepsilon_2\\varepsilon_1 \\mid X ]  \n",
    "        & \\mathbf{E} [  {\\varepsilon_2}^2 \\mid X ]          \n",
    "        & \\dots  \n",
    "        & \\mathbf{E} [ \\varepsilon_2\\varepsilon_m \\mid X ]  \n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots              \n",
    "        \\\\\n",
    "          \\mathbf{E} [ \\varepsilon_m\\varepsilon_1 \\mid X ]  \n",
    "        & \\mathbf{E} [ \\varepsilon_m\\varepsilon_2 \\mid X ] \n",
    "        & \\dots  \n",
    "        & \\mathbf{E} [ {\\varepsilon_m}^2 \\mid X ]\n",
    "    \\end{bmatrix}_\\textit{ m x m }\n",
    "    \\newline \\newline\n",
    "    &= \\quad \n",
    "    \\begin{bmatrix}\n",
    "          \\mathrm{Var}(\\varepsilon_1 \\mid X) \n",
    "        & \\mathrm{Cov}(\\varepsilon_1\\varepsilon_2 \\mid X)\n",
    "        & \\dots\n",
    "        & \\mathrm{Cov}(\\varepsilon_1\\varepsilon_m \\mid X)\n",
    "        \\\\\n",
    "          \\mathrm{Cov}(\\varepsilon_2\\varepsilon_1 \\mid X)\n",
    "        & \\mathrm{Var}(\\varepsilon_2 \\mid X) \n",
    "        & \\dots\n",
    "        & \\mathrm{Cov}(\\varepsilon_2\\varepsilon_m \\mid X)\n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots              \n",
    "        \\\\\n",
    "          \\mathrm{Cov}(\\varepsilon_m\\varepsilon_1 \\mid X)\n",
    "        & \\mathrm{Cov}(\\varepsilon_m\\varepsilon_2 \\mid X)\n",
    "        & \\dots \n",
    "        & \\mathrm{Var}(\\varepsilon_m \\mid X) \n",
    "    \\end{bmatrix}\n",
    "    \\newline \\newline\n",
    "    &= \\quad \n",
    "    \\begin{bmatrix}\n",
    "        \\sigma^2  &  0         &  \\dots   &  0         \\\\\n",
    "        0         &  \\sigma^2  &  \\dots   &  0         \\\\\n",
    "        \\vdots    &  \\vdots    &  \\vdots  &  \\vdots    \\\\\n",
    "        \\vdots    &  \\vdots    &  \\vdots  &  \\vdots    \\\\\n",
    "        0         &  0         &  \\dots   &  \\sigma^2  \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\quad = \\quad \n",
    "    \\sigma^2 I\n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we know that under the mentioned assumptions, we can re-write the covariance matrix of \n",
    "$\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}$ as :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{V}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}) \n",
    "        \\newline\n",
    "        &= \n",
    "            (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\n",
    "            \\ \\mathbf{E} \\big[ \\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top} \\big] \\         \n",
    "            \\mathbf{X} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}  \n",
    "            & \\text{by spherical errors (} \\textbf{A3 + A4} \\text{)}\n",
    "        \\newline\n",
    "        &= \n",
    "            (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}\n",
    "            \\ \\boldsymbol{\\sigma^2I} \\ \\\n",
    "            \\mathbf{X} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}  \n",
    "        \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \n",
    "            \\ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \n",
    "            \\ \\mathbf{X}^{\\top} \\mathbf{X} \\\n",
    "            (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}  \n",
    "        \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \n",
    "            & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            [\\textbf{E3}]\n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#28B463'>Variance of the OLS Estimators\n",
    "\n",
    "<br>\n",
    "It is easy to see that the covariance matrix of the OLS estimators is a function of (among other factors) both the variance and the covariance of the estimators themselves. Our goal, for the moment, is to compute these two formulas so that we will have a more detailed representation of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The notation below is based on regression residuals $\\boldsymbol{\\varepsilon_i}$ : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \n",
    "        &=\n",
    "            & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\text{by definition of variance}\n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big) ^2\n",
    "            \\ \\Big]  \n",
    "            & \\text{by } \\textbf{E1}\n",
    "        \\newline\n",
    "        &= \\mathbf{E} \\Big[ \\ \\big( \\sum_{i=1}^{N} \\boldsymbol{\\mathbf{c}_i}\\boldsymbol{\\varepsilon_i} \\big) ^2 \\ \\Big] \n",
    "        \\newline\n",
    "        &=  \\mathbf{E} \n",
    "            \\Big[\n",
    "                  \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{c}_i}^2 \\boldsymbol{\\varepsilon_i}^2\n",
    "                + 2 \\sum_{i=1}^{m} \\sum_{j \\neq i}^{m} \n",
    "                  \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{c}_j} \\boldsymbol{\\varepsilon_i} \\boldsymbol{\\varepsilon_s}\n",
    "            \\Big]\n",
    "            & [\\textbf{E4}]\n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Just like its equivalent in matrix form, this last equation can be furtherly simplified under the assumption of spherical errors :  \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \n",
    "        &=\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "            & \\text{by } \\textbf{E3}\n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[\n",
    "                  \\sum_{i=1}^{m} \\boldsymbol{{\\mathbf{c}_i}^2} \\boldsymbol{\\varepsilon_i}^2\n",
    "                + 2 \\sum_{i=1}^{m} \\sum_{j \\neq i}^{m} \n",
    "                  \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{c}_j} \\boldsymbol{\\varepsilon_i} \\boldsymbol{\\varepsilon_s}\n",
    "            \\Big]\n",
    "            & \\text{conditioning on } \\mathbf{X}\n",
    "        \\newline\n",
    "        &=  \\sum_{i=1}^{m} \\boldsymbol{{\\mathbf{c}_i}^2} \n",
    "                \\mathbf{E} \\Big[ \\boldsymbol{{\\varepsilon_i}^2} \\mid \\boldsymbol{\\mathbf{X}_i} \\Big]\n",
    "            + 2 \\sum_{i=1}^{m} \\sum_{j \\neq i}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{c}_j} \n",
    "                \\mathbf{E} \n",
    "                \\Big[   \n",
    "                    \\boldsymbol{\\varepsilon_i} , \\boldsymbol{\\varepsilon_j} \n",
    "                    \\mid \\boldsymbol{\\mathbf{X}_i} , \\boldsymbol{\\mathbf{X}_j}\n",
    "                \\Big]\n",
    "        \\newline\n",
    "        &=  \\sum_{i=1}^{m} \\boldsymbol{{\\mathbf{c}_i}^2} \n",
    "                \\mathrm{Var} \\Big( \\boldsymbol{\\varepsilon_i} \\mid \\boldsymbol{\\mathbf{X}_i} \\Big)\n",
    "            + 2 \\sum_{i=1}^{m} \\sum_{j \\neq i}^{m} \\boldsymbol{\\mathbf{c}_i} \\boldsymbol{\\mathbf{c}_j} \n",
    "                \\mathrm{Cov} \n",
    "                \\Big(\n",
    "                    \\boldsymbol{\\varepsilon_i} , \\boldsymbol{\\varepsilon_j} \n",
    "                    \\mid \\boldsymbol{\\mathbf{X}_i} , \\boldsymbol{\\mathbf{X}_j}\n",
    "                \\Big) \n",
    "                & \\text{by } \\textbf{A3} \\text{ and } \\textbf{A4}\n",
    "        \\newline\n",
    "        &=  \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \\boldsymbol{{\\mathbf{c}_i}^2}\n",
    "            & \\text{by } \\textbf{P2}\n",
    "        \\newline\n",
    "        &=  \\dfrac\n",
    "                { \\boldsymbol{\\sigma^2} } \n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) }\n",
    "            & [\\textbf{E5}]\n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#28B463'>Estimation of the variance\n",
    "\n",
    "<br>\n",
    "It's important to notice that, since the variance of the error terms $\\boldsymbol{\\sigma^2}$ is unobservable (being the error terms unobservable themselves), we will actually compute an estimate $\\boldsymbol{s^2}$ of it, based on the regression residuals :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{s^2} \n",
    "    \\ = \\ \\dfrac\n",
    "        { \\sum_{i=1}^{m} \\boldsymbol{{e_i}^2} }\n",
    "        { m - p }\n",
    "    \\ = \\ \\dfrac\n",
    "        { \\sum_{i=1}^{m} \\boldsymbol{e}^{\\top}\\boldsymbol{e} }\n",
    "        { m - p }\n",
    "    \\ = \\ \\dfrac\n",
    "        { \\text{SSR} }\n",
    "        { m - p }\n",
    "    \\qquad \\Rightarrow \\qquad\n",
    "    \\widehat{\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})} = \\boldsymbol{s^2} \\ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance of the OLS Estimators\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}, \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \n",
    "        &=\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "            & \\text{by definition of covariance}\n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0} ] \n",
    "                \\big)\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)\n",
    "            \\ \\Big]  \n",
    "            & \\text{by definition of } \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "                    - \\mathbf{E} [ \\overline{\\mathbf{Y}} - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\overline{\\mathbf{X}} ] \n",
    "                \\big)\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)\n",
    "            \\ \\Big]  \n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "                    - \\overline{\\mathbf{Y}} + \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \\overline{\\mathbf{X}} \n",
    "                \\big)\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)\n",
    "            \\ \\Big]  \n",
    "            & \\text{by definition of } \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}\n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\\n",
    "                \\big(\n",
    "                      \\overline{\\mathbf{Y}} - \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\overline{\\mathbf{X}}\n",
    "                    - \\overline{\\mathbf{Y}} + \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \\overline{\\mathbf{X}} \n",
    "                \\big)\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)\n",
    "            \\ \\Big]  \n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\ \n",
    "                - \\overline{\\mathbf{X}} \n",
    "                \\big( \n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)\n",
    "                \\big(\n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)\n",
    "            \\ \\Big]    \n",
    "        \\newline\n",
    "        &= \\mathbf{E} \n",
    "            \\Big[ \\ \n",
    "                - \\overline{\\mathbf{X}} \n",
    "                \\big( \n",
    "                      \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \n",
    "                    - \\mathbf{E} [\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} ] \n",
    "                \\big)^2\n",
    "            \\ \\Big]  \n",
    "        \\newline\n",
    "        &=  - \\overline{\\mathbf{X}} \\ \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})\n",
    "            & [\\textbf{E6}]\n",
    "    \\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal representation of the Covariance Matrix\n",
    "\n",
    "<br>\n",
    "<b>E2</b> and <b>E4</b> give the variance of the OLS estimator in the general case : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\mathrm{V}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}) \n",
    "    \\quad = \\quad\n",
    "    \\begin{bmatrix}\n",
    "          \\mathrm{Var}(\\hat{\\beta}_{OLS-1}) \n",
    "        & \\mathrm{Cov}(\\hat{\\beta}_{OLS-1} , \\hat{\\beta}_{OLS-2})\n",
    "        & \\dots\n",
    "        & \\mathrm{Cov}(\\hat{\\beta}_{OLS-1} , \\hat{\\beta}_{OLS-m})\n",
    "        \\\\\n",
    "          \\mathrm{Cov}(\\hat{\\beta}_{OLS-2} , \\hat{\\beta}_{OLS-1})\n",
    "        & \\mathrm{Var}(\\hat{\\beta}_{OLS-2}) \n",
    "        & \\dots\n",
    "        & \\mathrm{Cov}(\\hat{\\beta}_{OLS-2} , \\hat{\\beta}_{OLS-m})\n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots              \n",
    "        \\\\\n",
    "          \\mathrm{Cov}(\\hat{\\beta}_{OLS-m} , \\hat{\\beta}_{OLS-1})\n",
    "        & \\mathrm{Cov}(\\hat{\\beta}_{OLS-m} , \\hat{\\beta}_{OLS-2})\n",
    "        & \\dots \n",
    "        & \\mathrm{Var}(\\hat{\\beta}_{OLS-m}) \n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "<br>\n",
    "while <b>E3</b> and <b>E5</b> are extended, simplified versions that arise under the assumption of spherical errors : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\mathrm{V}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS}) \n",
    "    \\quad = \\quad\n",
    "    \\begin{bmatrix}\n",
    "          \\sigma^2 {(X^{\\top}X)^{-1}}_{11} \n",
    "        & 0 \n",
    "        & \\dots \n",
    "        & 0\n",
    "        \\\\\n",
    "          0  \n",
    "        & \\sigma^2 {(X^{\\top}X)^{-1}}_{22}  \n",
    "        & \\dots \n",
    "        & 0\n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\n",
    "        \\\\ \n",
    "        0 & 0 & \\dots & \\sigma^2 {(X^{\\top}X)^{-1}}_{mm} \n",
    "    \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the covariance matrix\n",
    "\n",
    "<br>\n",
    "$\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})$ and $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0})$ measure the statistical precision of the corresponding OLS estimators : \n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})\n",
    "    \\quad = \\quad\n",
    "    \\boldsymbol{\\sigma^2} \\ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\n",
    "    \\qquad \\qquad = \\quad\n",
    "    \\dfrac\n",
    "           { \\boldsymbol{\\sigma^2} } \n",
    "           { \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) }\n",
    "$\n",
    "\n",
    "$\n",
    "    \\quad    \n",
    "    \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0})\n",
    "    \\quad = \\quad\n",
    "    \\dfrac\n",
    "        { \\boldsymbol{\\sigma^2} }\n",
    "        { N }\n",
    "    \\dfrac\n",
    "        { \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i})^2 }\n",
    "        { \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2 }\n",
    "$\n",
    "\n",
    "<br>\n",
    "The variance of the two OLS coefficient estimators is smaller : \n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        the smaller is the (unobservable) variance of the disturbance terms $\\boldsymbol{\\sigma^2}$\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the larger is the variation of the sample values $\\boldsymbol{\\mathbf{X}_i}$ about their sample mean\n",
    "        $\\overline{\\mathbf{X}}$\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the larger is the sample size $N$\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br><br>\n",
    "Under the assumption of spherical errors, equation <b>E6</b> can be simplified into :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}, \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})\n",
    "        &= \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "            & \\text{by } \\textbf{E6}\n",
    "        \\newline\n",
    "        &=  \\ - \\overline{\\mathbf{X}} \\ \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})\n",
    "            & \\text{by } \\textbf{E5}\n",
    "        \\newline\n",
    "        &=  \\ - \\overline{\\mathbf{X}} \\\n",
    "            \\dfrac\n",
    "                { \\boldsymbol{\\sigma^2} } \n",
    "                { \\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}) }\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "Since both the numerator and the denominator of $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})$ are positive, the sign of $\\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0}, \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})$ depends on the\n",
    "sign of the sample mean :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        $ \n",
    "            \\overline{\\mathbf{X}} > 0 \n",
    "            \\quad \\Rightarrow \\quad \n",
    "            \\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0} , \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) > 0 \\quad \n",
    "        $\n",
    "        , the sampling errors $(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} - \\boldsymbol{\\beta_1})$ and\n",
    "        $(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} - \\boldsymbol{\\beta_1})$ are of opposite sign\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $ \n",
    "            \\overline{\\mathbf{X}} < 0 \n",
    "            \\quad \\Rightarrow \\quad \n",
    "            \\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-0} , \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) < 0 \\quad \n",
    "        $\n",
    "        , the sampling errors $(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} - \\boldsymbol{\\beta_1})$ and\n",
    "        $(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} - \\boldsymbol{\\beta_1})$ are of the same sign\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GM3] Proof of Efficiency\n",
    "\n",
    "<br>\n",
    "In this section of the notebook we will demonstrate that the OLS estimators has the minimum variance in the class of all the linear unbiased estimators of the population parameters. \n",
    "\n",
    "<br>\n",
    "Recall that $\\quad$ <b>Efficiency = Unbiasedness + Minimum Variance</b>\n",
    "\n",
    "<br>\n",
    "The demonstration is structured in three points :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        the definition of an arbitrary estimator $\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1}$ linear in $\\mathbf{Y}$\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the imposition on $\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1}$ of restrictions implied by unbiasedness\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        the core of the demonstration, where we will show that the variance of the arbitrary estimator\n",
    "        $\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1}$ must be larger than, or at least equal to, the variance of\n",
    "        $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Let's define an arbitrary estimator $\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1}$ linear in $\\mathbf{Y}$ :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad \\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1} \n",
    "    = \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \\boldsymbol{\\mathbf{Y}_i}\n",
    "    = \n",
    "        \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \n",
    "        (\\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1}\\boldsymbol{\\mathbf{X}_i} + \\boldsymbol{\\varepsilon_i})\n",
    "    = \n",
    "        \\boldsymbol{\\beta_0} \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \n",
    "        + \\boldsymbol{\\beta_1} \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \\boldsymbol{\\mathbf{X}_i}\n",
    "        + \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \\boldsymbol{\\varepsilon_i}\n",
    "$\n",
    "\n",
    "<br>\n",
    "A quick look at <b>GM1</b> will remind us that for the estimator $\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1}$ to be unbiased, the following restrictions must be accomplished :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} = 0 \n",
    "    \\quad \\text{and} \\quad \n",
    "    \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \\boldsymbol{\\mathbf{X}_i}= 1\n",
    "    \\quad \\Rightarrow \\quad\n",
    "    \\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{1} \n",
    "    = \\boldsymbol{\\beta_1} + \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i} \\boldsymbol{\\varepsilon_i}        \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We will now show that the OLS estimator $\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{1}$ has the minimum variance in the class of all the linear unbiased estimator of $\\boldsymbol{\\beta_1}$ :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{Var}(\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \n",
    "        &=\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            & \\text{by definition of variance}\n",
    "        \\newline\n",
    "        &=\n",
    "            \\mathbf{E} \n",
    "            \\big[ \\\n",
    "                \\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \n",
    "                - \\mathbf{E} [\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}] \n",
    "            \\ \\big]^2\n",
    "            & \\text{by unbiasedness of } \\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \\big[ \\ \\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} - \\boldsymbol{\\beta_1} \\ \\big]^2\n",
    "            & \\text{by } \\textbf{E1}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\mathbf{E} \\Big[ \\ \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{h}_i}\\boldsymbol{\\varepsilon_i} \\ \\Big] ^2\n",
    "            = \\sum_{i=1}^{m} \\Big[ \\boldsymbol{{\\mathbf{h}_i}^2} \\ \\mathbf{E} [ \\boldsymbol{\\varepsilon_i} ] ^2 \\Big]\n",
    "            = \\boldsymbol{\\sigma^2} \\ \\sum_{i=1}^{m} \\boldsymbol{{\\mathbf{h}_i}^2}\n",
    "        \\newline  \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}\n",
    "            \\Big[ \\\n",
    "                 (\\boldsymbol{\\mathbf{h}_i} - \\boldsymbol{\\mathbf{c}_i} + \\boldsymbol{\\mathbf{c}_i}) ^2 \n",
    "            \\ \\Big] \n",
    "        \\newline\n",
    "        &= \n",
    "              \\boldsymbol{\\sigma^2}   \\sum_{i=1}^{m} \\Big[ \\ (\\boldsymbol{\\mathbf{h}_i} - \\boldsymbol{\\mathbf{c}_i})^2 \\ \\Big] \n",
    "            + \\boldsymbol{\\sigma^2}   \\sum_{i=1}^{m} \\Big[ \\ {\\boldsymbol{\\mathbf{c}_i}} ^2 \\ \\Big]  \n",
    "            + 2 \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \n",
    "                \\Big[ \\ (\\boldsymbol{\\mathbf{h}_i} - \\boldsymbol{\\mathbf{c}_i})  \\boldsymbol{\\mathbf{c}_i} \\ \\Big] \n",
    "        \\newline  \n",
    "        &= \n",
    "              \\boldsymbol{\\sigma^2}   \\sum_{i=1}^{m} \\Big[ \\ (\\boldsymbol{\\mathbf{h}_i} - \\boldsymbol{\\mathbf{c}_i})^2 \\ \\Big] \n",
    "            + \\boldsymbol{\\sigma^2}   \\sum_{i=1}^{m} \\Big[ \\ {\\boldsymbol{\\mathbf{c}_i}} ^2 \\ \\Big]  \n",
    "            + 2 \\ \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \\Big[ \\ \\boldsymbol{\\mathbf{h}_i} \\boldsymbol{\\mathbf{c}_i} \\ \\Big]\n",
    "            - 2 \\ \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \\Big[ \\ {\\boldsymbol{\\mathbf{c}_i}} ^2 \\ \\Big] \n",
    "        \\newline \n",
    "        \\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "    \\quad\n",
    "    \\qquad \\qquad \\ \\\n",
    "    \\begin{align}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \n",
    "            \\Bigg[ \n",
    "                \\boldsymbol{\\mathbf{h}_i} \n",
    "                - \\dfrac\n",
    "                    {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                    {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\Bigg] ^2\n",
    "            + \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}  \n",
    "            \\Bigg[ \n",
    "                \\dfrac\n",
    "                    {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                    {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2} \n",
    "            \\Bigg] ^2 \n",
    "        \\newline\n",
    "        &\n",
    "            \\quad\n",
    "            + 2 \\boldsymbol{\\sigma^2} \\sum_{i=1m}   \n",
    "                \\Bigg[              \n",
    "                    \\boldsymbol{\\mathbf{h}_i} \\\n",
    "                    \\dfrac\n",
    "                        {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                        {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "                \\Bigg] \n",
    "            - 2 \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}    \n",
    "                \\Bigg[ \n",
    "                        \\dfrac\n",
    "                            {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                            {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2} ^2\n",
    "                \\Bigg] \n",
    "        \\newline \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}\n",
    "            \\Bigg[\n",
    "                \\boldsymbol{\\mathbf{h}_i} \n",
    "                    - \\dfrac\n",
    "                        {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                        {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\Bigg] ^2\n",
    "            + \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \n",
    "            \\Bigg[\n",
    "                \\dfrac\n",
    "                    {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                    {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}                \n",
    "            \\Bigg] ^2 \n",
    "            + 2 \\ \\boldsymbol{\\sigma^2} - 2 \\ \\boldsymbol{\\sigma^2}\n",
    "        \\newline \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}\n",
    "            \\Bigg[\n",
    "                \\boldsymbol{\\mathbf{h}_i} \n",
    "                    - \\dfrac\n",
    "                        {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                        {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\Bigg] ^2\n",
    "            + \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m} \n",
    "            \\Bigg[\n",
    "                \\dfrac\n",
    "                    {\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}}}\n",
    "                    {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}                \n",
    "            \\Bigg] ^2 \n",
    "            & \\text{by } \\textbf{P2}\n",
    "        \\newline \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}\n",
    "                \\Big[ \\ (\\boldsymbol{\\mathbf{h}_i} - \\boldsymbol{\\mathbf{c}_i}) ^2 \\ \\Big] \n",
    "            + \\dfrac\n",
    "                { \\boldsymbol{\\sigma^2} }\n",
    "                {\\sum_{i=1}^{m} (\\boldsymbol{\\mathbf{X}_i} - \\overline{\\mathbf{X}})^2}\n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \n",
    "            \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \n",
    "            & \\text{by } \\textbf{E5}\n",
    "        \\newline\n",
    "        &= \n",
    "            \\boldsymbol{\\sigma^2} \\sum_{i=1}^{m}\n",
    "                \\Big[ \\ (\\boldsymbol{\\mathbf{h}_i} - \\boldsymbol{\\mathbf{c}_i}) ^2 \\ \\Big] \n",
    "            + \\mathrm{Var}(\\ \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} \\ )\n",
    "    \\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "The first term (on the right-hand side) of the last equation will always be positive, being a sum of squares; the only exception is when $\\boldsymbol{\\mathbf{h}_i} = \\boldsymbol{\\mathbf{c}_i}$, in other words when $\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1} = \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}$, in this circumstance the two estimators have the same variance :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad \n",
    "    \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1})\n",
    "    \\quad \\leq \\quad\n",
    "    \\mathrm{Var}(\\tilde{\\boldsymbol{\\beta}}_\\boldsymbol{OLS-1}) \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Finite sample properties\n",
    "\n",
    "Under the assumption of strict exogeneity, the OLS estimators $\\hat{\\boldsymbol{\\beta}}$ and $\\hat{\\boldsymbol{\\sigma}}^2$ are unbiased, meaning that their expected values coincide with the true values of the population parameters :\n",
    "\n",
    "$\n",
    "\\mathbf{E} [\\hat{\\boldsymbol{\\beta}} \\mid \\mathbf{X}] = \\boldsymbol{\\beta} , \n",
    "\\quad \\mathbf{E} [\\hat{\\boldsymbol{\\sigma}}^2 \\mid \\mathbf{X}] = \\boldsymbol{\\sigma}^2\n",
    "$\n",
    "The estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors : \n",
    "$ {\\displaystyle \\operatorname {E} [\\,\\mathbf{y} {x} _{i}\\varepsilon _{i}\\,] = 0 } $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "{\n",
    "    \\displaystyle { \\boldsymbol {\\beta} } = \n",
    "        (\\mathbf {X} ^{\\top }\\mathbf {X} )^{-1} \n",
    "        \\mathbf {X} ^{\\top }\\mathbf {y} =\n",
    "    \\left(\\sum \\mathbf {x} _{i}\\mathbf {x} _{i}^{\\top }\\right)^{-1}\n",
    "    \\left(\\sum \\mathbf {x} _{i}y_{i}\\right)\n",
    "}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also efficient under the assumption that the errors have finite variance and are homoscedastic, meaning that \n",
    "$ {\\displaystyle \\operatorname {E} [\\,\\varepsilon_{i}^{2} | \\mathbf {x}_{i}\\,] = 0 } $ does not depend on i. \n",
    "The condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment, but in the case of observational data, it is difficult to exclude the possibility of an omitted covariate z that is related to both the observed covariates and the response variable. The existence of such a covariate will generally lead to a correlation between the regressors and the response variable, and hence to an inconsistent estimator of β. The condition of homoscedasticity can fail with either experimental or observational data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the goal is either inference or predictive modeling, the performance of OLS estimates can be poor if multicollinearity is present, unless the sample size is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple linear regression, where there is only one regressor (with a constant), the OLS coefficient estimates have a simple form that is closely related to the correlation coefficient between the covariate and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#28B463'>References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "         Queen's University at Kingston - Economics 351 - M.G. Abbott -\n",
    "         <a href=\"https://bit.ly/2IFUS3n\">\n",
    "         Statistical Properties of the OLS Coefficient Estimators</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "         University of Valencia - Ezequiel Uriel - \n",
    "         <a href=\"https://bit.ly/2x9cSh6\">\n",
    "         The simple regression model : estimation and properties</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Wake Forest University - Allin Cottrell - \n",
    "        <a href=\"https://bit.ly/2Ls3tV9\">\n",
    "        Regression Basics in Matrix terms</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Wikipedia - \n",
    "        <a href=\"https://bit.ly/2IKZelN\">\n",
    "        Gauss Markov Theorem</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Wikipedia - \n",
    "        <a href=\"https://bit.ly/2IJEWcc\">\n",
    "        Covariance Matrix</a>\n",
    "    </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
