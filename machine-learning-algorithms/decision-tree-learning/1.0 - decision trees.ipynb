{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "This notebook will present a family of general-purpose learning systems characterized by their representation of acquired knowledge as decision trees. Decision tree learning is a method for approximating discrete-valued target functions, in which the learned function is represented by a decision tree or, alternatively, as sets of if-then rules to improve human readability.\n",
    "\n",
    "<br>\n",
    "The systems described here are presented with a set of cases relevant to a classification task and develop a decision tree from the top down, guided by frequency information in the examples but not by the particular order in which the examples are given. These trees are constructed beginning with the root of the tree and proceeding down to its leaves, a technique which is emphasized by the algorithms family name : Top Down Induction Decision Trees. \n",
    "\n",
    "<br>\n",
    "The example objects from which a classification rule is developed are known only through their values of a set of properties or attributes, and the decision trees in turn are expressed in terms of these same attributes : each node in the tree specifies a test on a specific attribute, and each branch descending from that node corresponds to one of the possible values for the attribute at hand. Finally, the leaves of a decision tree correspond to the class names.\n",
    "\n",
    "<br>\n",
    "The decision tree in the picture below, for example, classifies saturday mornings according to whether they are suitable for playing tennis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='decision_trees_mitchell_outlook'>\n",
    "    <img src=\"images/mitchell_outlook.jpg\" alt=\"outlook\" width=\"50%\" height=\"50%\">\n",
    "</a>\n",
    "\n",
    "<br>\n",
    "<center><b>Figure 1</b> : a decision tree for the concept PlayTennis</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Types\n",
    "\n",
    "<br>\n",
    "Decision trees are of two main types :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>classification trees</b> : the target variable can take a discrete set of values; in these tree structures, leaves\n",
    "        represent class labels and branches represent conjunctions of features that lead to those class labels\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>regression trees</b> : the target variable can take continuous values (usually real numbers) \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "The term <b>CART (Classification And Regression Tree)</b> is an umbrella term used to refer to both of the above tree structures; classification trees and regression trees have some similarities but also some differences, such as the procedure used to determine where to split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Induction Task\n",
    "\n",
    "<br>\n",
    "The basis is a universe of objects that are described in terms of a collection of attributes; each attribute measures some important feature of an object and will be limited here to taking a (usually small) set of discrete, mutually exclusive values. \n",
    "\n",
    "<br>\n",
    "Each object in the universe belongs to one of a set of mutually exclusive classes; to further simplify the treatment of the dataset we'll be exploring in the notebook, we will assume that there are only two such classes, denoted P and N. In binary classification problems, objects of class P and N are often referred to as positive and negative instances, respectively. \n",
    "\n",
    "<br>\n",
    "Given a training set of objects whose class is known, the induction task is to develop a classification rule that can determine the class of any object from the values of the its attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| #  | Outlook  | Temperature | Humidity | Windy | Play  |\n",
    "| --:|:-------- |:----------- |:-------- |:----- |:-----:|\n",
    "|  1 | sunny    | hot         | high     | false | N     |\n",
    "|  2 | sunny    | hot         | high     | true  | N     |\n",
    "|  3 | overcast | hot         | high     | false | P     |\n",
    "|  4 | rain     | mild        | high     | false | P     |\n",
    "|  5 | rain     | cool        | normal   | false | P     |\n",
    "|  6 | rain     | cool        | normal   | true  | N     |\n",
    "|  7 | overcast | cool        | normal   | true  | P     |\n",
    "|  8 | sunny    | mild        | high     | false | N     |\n",
    "|  9 | sunny    | cool        | normal   | false | P     |\n",
    "| 10 | rain     | mild        | normal   | false | P     |\n",
    "| 11 | sunny    | mild        | normal   | true  | P     |\n",
    "| 12 | overcast | mild        | high     | true  | P     |\n",
    "| 13 | overcast | hot         | normal   | false | P     |\n",
    "| 14 | rain     | mild        | high     | true  | N     |\n",
    "\n",
    "<br>\n",
    "<center><b>Table 1</b> : a small training set</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute in the given example; the process continues until a leaf is encountered, at which time the object is asserted to belong to the class named by the leaf. Notice that only a subset of the attributes may be encountered on a particular path from the root of the decision tree to a leaf.\n",
    "\n",
    "<br>\n",
    "Assuming the assumption that the attributes are \"adequate\" (objects described by identical values of the properties belong to the same class), it is always possible to construct a decision tree that correctly classifies each object in the training set, and usually there are many such correct decision trees. The essence of induction is to move beyond the training set, i.e. to construct a decision tree that correctly classifies not only objects from the training set but other (unseen) objects as well. In order to do this, the decision tree must capture some meaningful relationship between an object's class and its values of the attributes. \n",
    "\n",
    "<br>\n",
    "Given a choice between two decision trees, each of which is correct over the training set and with comparable performances, it seems sensible to prefer the simpler one on the grounds that it is just as capable of capturing the relevant patterns within the data, but less prone to overfitting. The simpler tree is expected to classify correctly more objects outside the training set. \n",
    "\n",
    "<br>\n",
    "The decision tree of Figure 3, for instance, is also correct for the training set of Table 1, but its greater complexity makes it suspect as an 'explanation' of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"45.5%\"><img src=\"images/induction_of_decision_trees_figure_2.png\" alt=\"a simple decision tree\"></td>\n",
    "        <td width=\"54.5%\"><img src=\"images/induction_of_decision_trees_figure_3.png\" alt=\"a complex decision tree\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><center><b>Figure 2</b> : a simple decision tree</center></th>\n",
    "        <th><center><b>Figure 3</b> : a complex decision tree</center></th>\n",
    "    </tr>    \n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appropriate Problems\n",
    "\n",
    "<br>\n",
    "Although a variety of decision tree learning methods have been developed (with somewhat differing capabilities and requirements), decision trees are generally best suited to problems with the following characteristics :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>instances are represented by attribute-value pairs</b> : instances are described by a fixed set of attributes (e.g.\n",
    "        Temperature) and their values (e.g., Hot). The easiest situation for decision tree learning is when each attribute takes\n",
    "        on a small number of disjoint possible values (e.g. Hot, Mild, Cold); however, <b>extensions to the basic algorithm\n",
    "        allow handling real-valued attributes as well</b>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>the target variable has discrete output values</b> : the decision tree in the picture above, for example, assigns a\n",
    "        boolean classification to each instance; decision tree methods easily extend to learning functions with more than two\n",
    "        possible output values. <b>A more substantial extension allows learning target functions with real-valued outputs</b>,\n",
    "        though the application of decision trees in this setting is less common\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>disjunctive descriptions may be required</b> : decision trees naturally represent disjunctive expressions\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>the training data may contain errors</b> : decision tree learning methods are robust to errors, both errors in\n",
    "        classifications of the training examples and errors in the attribute values that describe these examples\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>the training data may contain missing attribute values</b> : decision tree methods can be used even when some\n",
    "        training examples have unknown values\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "<br>\n",
    "This is a relatively simple knowledge formalism that lacks the expressive power of semantic networks or other first-order representations. As a consequence of this simplicity, the learning methodologies used in the TDIDT family are considerably less complex than those employed in systems that can express the results of their learning in a more powerful language. Nevertheless, it is still possible to generate knowledge in the form of decision trees that is capable of solving difficult problems of practical significance.\n",
    "\n",
    "Top Down Induction Decision Trees are among the most popular of inductive inference algorithms\n",
    "and have been successfully applied to a broad range of tasks.\n",
    "\n",
    "\n",
    "<br>\n",
    "Decision tree learning is robust to noisy data and capable of learning disjunctive expressions; it's one of the most popular and practical algorithms for inductive inference and have been successfully applied to a broad range of tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "         Tom Mitchell - \n",
    "         <a href=\"https://bit.ly/2IYw6Yc\">\n",
    "         Machine Learning</a>        \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        J. Ross Quinlan - \n",
    "        <a href=\"https://bit.ly/2JCjrKM\">\n",
    "        Induction of Decision Trees</a>   \n",
    "    </li>  \n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
