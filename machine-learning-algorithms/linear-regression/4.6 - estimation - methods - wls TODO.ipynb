{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTED LEAST SQUARES\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<br>\n",
    "A special case of GLS called <b>Weighted Least Squares</b> (<b>WLS</b>) occurs when the disturbance terms are uncorrelated but still exhibit heteroscedasticity.\n",
    "\n",
    "<br>\n",
    "Given the following covariance matrix, where the off-diagonal entries of $\\mathbf{\\Sigma}$ are null but the remaining entries may still have different variances, we can compute a simpler version of the transformation matrix $\\mathbf{G}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\quad\n",
    "    \\mathrm{V}(\\boldsymbol{\\varepsilon}) \n",
    "    \\quad = \\quad\n",
    "    \\mathbf{\\Sigma}\n",
    "    \\quad = \\quad\n",
    "    \\begin{bmatrix}\n",
    "        {\\sigma_1}^2  &  0             &  \\dots   &  0             \\\\\n",
    "        0             &  {\\sigma_2}^2  &  \\dots   &  0             \\\\ \n",
    "        \\vdots        &  \\vdots        &  \\vdots  &  \\vdots        \\\\\n",
    "        \\vdots        &  \\vdots        &  \\ddots  &  \\vdots        \\\\ \n",
    "        0             &  0             &  \\dots   &  {\\sigma_m}^2 \n",
    "    \\end{bmatrix} _\\textit{ m x m }\n",
    "    \\quad = \\quad\n",
    "    \\boldsymbol{\\mathbf{W}^{-1}}\n",
    "    \\qquad \\Rightarrow \\qquad\n",
    "    \\mathbf{G} \n",
    "    \\quad = \\quad \\mathbf{\\Sigma}^{\\ -1/2} \n",
    "    \\quad = \\quad \n",
    "        \\begin{bmatrix}\n",
    "            {\\sigma_1}^{-1}  &  0                &  \\dots   &  0                \\\\\n",
    "            0                &  {\\sigma_2}^{-1}  &  \\dots   &  0                \\\\ \n",
    "            \\vdots           &  \\vdots           &  \\vdots  &  \\vdots           \\\\\n",
    "            \\vdots           &  \\vdots           &  \\ddots  &  \\vdots           \\\\ \n",
    "            0                &  0                &  \\dots   &  {\\sigma_m}^{-1}\n",
    "        \\end{bmatrix} _\\textit{ m x m }   \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Please note that, in the context of WLS, the inverse of the matrix $\\mathbf{\\Sigma}$ will be refferred to as $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization Objective\n",
    "\n",
    "<br>\n",
    "The WLS minimization objective can be seen as a particular version of its GLS analogous, with a simpler structure : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "    S_{WLS}(\\hat{\\boldsymbol{\\beta}}, \\mathbf{W}) \n",
    "    &=\n",
    "    \\newline\n",
    "    &=\n",
    "        \\boldsymbol{c^2}\n",
    "        \\ (\\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} )^{T} \n",
    "        \\ \\mathbf{\\Sigma}^{-1} \n",
    "        \\ ( \\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} ) \n",
    "    \\newline \n",
    "    &=\n",
    "        \\boldsymbol{c^2}\n",
    "        \\ (\\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} )^{T} \n",
    "        \\ \\mathbf{W} \n",
    "        \\ ( \\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} ) \n",
    "    \\newline \\newline\n",
    "    &= \\sum _{i=1}^{m} \\boldsymbol{\\mathbf{W}_{ii}}\n",
    "        \\big[ \\boldsymbol{\\mathbf{Y}_i} - (\\boldsymbol{\\mathbf{X}_i})^{\\top} \\boldsymbol{\\hat{\\beta}} \\big] ^\\boldsymbol{2}\n",
    "    \\quad = \\quad \\sum_{i=1}^{m} \\boldsymbol{\\mathbf{W}_i} \\ {\\boldsymbol{\\mathbf{e}_i}}^\\boldsymbol{2}\n",
    "    \\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WLS Estimators\n",
    "\n",
    "<br>\n",
    "And the same can be told of the WLS estimators :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\boldsymbol{\\hat{\\beta}_{WLS-1}}\n",
    "        &= \n",
    "        \\newline\n",
    "        &=  \\big[ \\mathbf{X}^{\\top} \\ \\mathbf{\\Sigma}^{-1} \\ \\mathbf{X} \\big] ^{-1} \n",
    "            \\mathbf{X}^{\\top} \\ \\mathbf{\\Sigma}^{-1} \\ \\mathbf{Y}   \n",
    "        \\newline\n",
    "        &=  \\big[ \\mathbf{X}^{\\top} \\ \\mathbf{W} \\ \\mathbf{X} \\big] ^{-1} \n",
    "            \\mathbf{X}^{\\top} \\ \\mathbf{W} \\ \\mathbf{Y}   \n",
    "    \\end{align}    \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{V}_{WLS}(\\boldsymbol{\\varepsilon^*}) \n",
    "        &= \n",
    "        \\newline\n",
    "        &= \\mathrm{V}_{GLS}(\\boldsymbol{\\varepsilon^*}) \n",
    "        \\newline\n",
    "        &= \\boldsymbol{c^2} \\boldsymbol{\\textit{I}} _\\textit{ m x m }\n",
    "    \\end{align}    \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{V}(\\boldsymbol{\\hat{\\beta}_{WLS-1}}) \n",
    "        &=\n",
    "        \\newline\n",
    "        &= \\big[ \\mathbf{X}^{\\top} \\ \\mathbf{\\Sigma}^{-1} \\ \\mathbf{X} \\big] ^{-1} \n",
    "        \\newline\n",
    "        &= \\big[ \\mathbf{X}^{\\top} \\ \\mathbf{W} \\ \\mathbf{X} \\big] ^{-1} \n",
    "    \\end{align}    \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\mathrm{V}_{WLS}(\\mathbf{Y^*}) \n",
    "        &= \n",
    "        \\newline\n",
    "        &= \\mathrm{V}_{GLS}(\\mathbf{Y^*})  \n",
    "        \\newline\n",
    "        & = \\boldsymbol{c^2} \\ \\boldsymbol{\\textit{I}} _\\textit{ m x m }       \n",
    "    \\end{align}    \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\quad\n",
    "    \\begin{align}\n",
    "        \\boldsymbol{{s^2}_{WLS}} \n",
    "        &=\n",
    "        \\newline\n",
    "        &=\n",
    "            \\dfrac{1}{m - p} \n",
    "            \\ \\boldsymbol{c^2}\n",
    "            \\ (\\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} )^{T} \n",
    "            \\ \\mathbf{\\Sigma}^{-1} \n",
    "            \\ ( \\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} ) \n",
    "        \\newline\n",
    "        &=\n",
    "            \\dfrac{1}{m - p} \n",
    "            \\ \\boldsymbol{c^2}\n",
    "            \\ (\\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} )^{T} \n",
    "            \\ \\mathbf{W} \n",
    "            \\ ( \\mathbf{Y} - \\mathbf{X} \\ \\hat{\\boldsymbol{\\beta}} ) \n",
    "    \\end{align}    \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can think of OLS estimation as a special case of WLS (and consequently of GLS) which not only has uncorrelated error terms, but also presents homoscedasticity ($\\boldsymbol{\\mathbf{W}_i} = 1 $).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Estimation TODO\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Estimation TODO\n",
    "\n",
    "<br>\n",
    "If there are replicates in the data, the most obvious way to estimate the weights is to set the weight for each data point equal to the reciprocal of the sample variance obtained from the set of replicate measurements to which the data point belongs. Mathematically :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{\\mathbf{W}_{ij}} \n",
    "    = \n",
    "        \\dfrac {1}{ \\boldsymbol{{\\sigma^2}_i} } = \n",
    "        \\big[ \n",
    "            \\dfrac\n",
    "            { \\sum_{j=1}^{m_i}(\\boldsymbol{\\mathbf{Y}_{ij}} - \\boldsymbol{\\overline{\\mathbf{Y}}_i})^2 } \n",
    "            {m_i - 1} \n",
    "        \\big]^{-1}\n",
    "$\n",
    "\n",
    "<br>\n",
    "where :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        $\\boldsymbol{\\mathbf{W}_{ij}}$ are the weights indexed by their regressor variable levels and replicate measurements\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $\\boldsymbol{i}$ indexes the unique combinations of regressor variable values, and $\\boldsymbol{j}$ indexes the\n",
    "        replicates within each combination of regressor variable values\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $\\boldsymbol{{\\sigma^2}_i}$ is the sample standard deviation of the response variable at the $i^\\text{th}$ combination\n",
    "        of regressor variable values\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $m_i$ is the number of replicate observations at the $i^\\text{th}$ combination of regressor variable values\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $\\boldsymbol{\\mathbf{Y}_{ij}}$ are the individual data points indexed by their regressor variable levels and replicate\n",
    "        measurements\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        $\\boldsymbol{\\overline{\\mathbf{Y}}_i}$ is the mean of the responses at the $i^\\text{th}$ combination of regressor\n",
    "        variable levels\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Unfortunately, although this method is attractive, it rarely works well. This is because when the weights are estimated this way, they are usually extremely variable. As a result, the estimated weights do not correctly control how much each data point should influence the parameter estimates. \n",
    "\n",
    "<br>\n",
    "This method can work, but it requires a very large number of replicates at each combination of predictor variables. In fact, if this method is used with too few replicate measurements, the parameter estimates can actually be more variable than they would have been if the unequal variation were ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better strategy TODO\n",
    "\n",
    "<br>\n",
    "A better strategy for estimating the weights is to find a function that relates the standard deviation of the response at each combination of regressor variable values to the regressor variables themselves :\n",
    "\n",
    "<br>\n",
    "$\n",
    "    \\quad\n",
    "    \\boldsymbol{ {\\hat{\\sigma}^2}_i } \\quad \\approx \\quad \\mathbf{g} (\\boldsymbol{\\mathbf{X}_i}, \\mathbf{\\gamma})\n",
    "$\n",
    "\n",
    "<br>\n",
    "This approach to estimating the weights usually provides more precise estimates than direct estimation because fewer quantities have to be estimated and there is more data to estimate each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "<br>\n",
    "It is fundamental to understand that, unlike linear and non-linear least squares regression, <b>WLS</b> regression is <b>not associated with a particular specification</b> of the functional form (the function used to describe the relationship between the dependent and independent variables). \n",
    "\n",
    "<br>\n",
    "Instead, WLS reflects the behavior of the disturbance terms, and it can be used with functions that are either linear or non-linear in the parameters. It works by incorporating extra non-negative constants (weights), into the fitting criterion. \n",
    "\n",
    "<br>\n",
    "Optimizing the weighted fitting criterion to find the parameter estimates allows the weights to determine the contribution of each observation to the final parameter estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Weighted Least Squares estimation is particularly relevant when our aim is :\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">\n",
    "    <li>\n",
    "        <b>focusing accuracy</b> <br>\n",
    "        [<b>C1</b>]\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>discounting imprecision</b> <br>\n",
    "        [<b>C2</b>]\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <b>solving other optimization problems</b> <br>\n",
    "        [<b>C3</b>]\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [C1] Focusing accuracy\n",
    "\n",
    "<br>\n",
    "Depending on the context of the regression, we may want to dedicate more attention to the predicted response for specific values of the regressors : values we expect to see often again, associated to mistakes that are especially costly than others. \n",
    "\n",
    "<br>\n",
    "When assigning a larger weight to the $i^\\text{th}$ residual (or to a certain set of residuals), we are modifying the objective function so that the regression hyperplane will be pulled towards that specific observation of the response variable. In other words, the regression is trying harder to fit a certain set of observation, while \"neglecting\" others with smaller weights.\n",
    "\n",
    "<br>\n",
    "In this first context, we choose the weights to reflect our priorities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [C2] Discounting imprecision\n",
    "\n",
    "<br>\n",
    "As we already know, OLS corresponds to Maximum Likelihood Estimation when the disturbance term $\\boldsymbol{\\varepsilon}$ is IID Gaussian white noise. This means that, with the disturbance term having constant variance, we are measuring the regression curve with the same precision everywhere. \n",
    "\n",
    "<br>\n",
    "However, the magnitude of the noise is often not constant, and the data are said to exhibit heteroscedasticity. In presence of heteroskedasticity, even if each and every noise term is still Gaussian, OLS is no longer efficient, and in fact no longer match MLE.\n",
    "\n",
    "<br>\n",
    "We also know that not all is lost; if the covariance matrix of the disturbance term is known (or can be estimated through OLS regression residuals), we can use WLS to recover the efficiency of parameter estimation. This is done by \"modulating\" the objective function in such a way that each data point has the proper amount of influence over the parameter estimation process. \n",
    "\n",
    "<br>\n",
    "There is no way we can estimate the regression function as accurately where the noise is large as we can where the noise is small; <b>a procedure that treats all of the observations equally would give to high-variance (or less precisely measured) points more influence than they should have, and too little influence to small-variance points</b>.\n",
    "\n",
    "<br>\n",
    "Trying to give equal attention to all parts of the input space is a waste of resources; our aim should be fitting well where the noise is small (observations measured with high precision), and expect to fit poorly where the noise is large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [C3] Solving other optimization problems\n",
    "\n",
    "<br>\n",
    "There are a number of other optimization problems which can be transformed into (or approximated by) WLS. The most important of these arises from Generalized Linear Models, where the mean response is some non-linear function of a linear predictor (logistic regression is an example.)\n",
    "\n",
    "<br>\n",
    "In this third case, the weights come from the optimization problem we are actually in the process of solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "\n",
    "<br>\n",
    "Like all of the least squares methods discussed so far, weighted least squares <b>is an efficient method that makes good use of (it extracts more information from) small data sets</b>. It also shares the ability to provide different types of easily interpretable statistical intervals for estimation, prediction, calibration and optimization. \n",
    "\n",
    "<br>\n",
    "In addition, as discussed above, the main advantage that weighted least squares enjoys over other methods is the <b>ability to handle regression situations in which the data points are of varying quality</b>. If the standard deviation of the random errors in the data is not constant across all levels of the explanatory variables, using weighted least squares with proper weights yields the most precise parameter estimates possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "\n",
    "<br>\n",
    "The biggest disadvantage of weighted least squares is the fact that the theory behind <b>this method is based on the assumption that the weights are known exactly</b>. This is almost never the case in real applications, of course, so estimated weights must be used instead. \n",
    "\n",
    "<br>\n",
    "The effect of using estimated weights is difficult to assess. Experience indicates that small variations in the the weights due to estimation do not often affect a regression analysis or its interpretation. If the weights can be estimated with high enough precision, their use can significantly improve the parameter estimates compared to the results that would be obtained if all of the data points were equally weighted. However, when the weights are estimated from small numbers of replicated observations, the results of an analysis can be very badly and unpredictably affected. \n",
    "\n",
    "<br>\n",
    "This is especially likely to be the case when the weights for extreme values of the regressors are estimated using only a few observations. It is important to remain aware of this potential problem, and to only use weighted least squares when the weights can be estimated precisely relative to one another.\n",
    "\n",
    "<br>\n",
    "Weighted Least Squares regression, like the other least squares methods, is also sensitive to the <b>effects of outliers and high-leverage points</b>. If potential outliers are not investigated and dealt with appropriately, they will likely have a negative impact on the parameter estimation and other aspects of a weighted least squares analysis. If a weighted least squares regression actually increases the influence of an outlier, the results of the analysis may be far inferior to an unweighted least squares analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives TODO\n",
    "\n",
    "<br>\n",
    "It is wise to use WLS only when the weights can be estimated with high precision. When dealing with heteroscedasticity, it is common to run OLS instead, and use a different variance estimator. \n",
    "\n",
    "<br>\n",
    "While White’s consistent estimator doesn’t require heteroscedasticity, it isn’t a very efficient strategy. However, if you don’t know the weights for your data, it may be your best choice. For a full explanation of how to implement White’s consistent estimator, you can read White’s original 1908 paper for free here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "<ul style=\"list-style-type:square\">    \n",
    "    <li>\n",
    "        National Taiwan University - Chung-Ming Kuan - \n",
    "        <a href=\"https://bit.ly/2IJ3hmG\">\n",
    "        Generalized Least Squares Theory</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Carleton University - Ba Chu - \n",
    "        <a href=\"https://bit.ly/2J9PpB4\">\n",
    "        Generalized Least Squares Theory</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        National Institute of Standards and Technology - \n",
    "        <a href=\"https://bit.ly/2GJ8tRD\">\n",
    "        Accounting for Non-Constant Variation Across the Data TODO</a>\n",
    "    </li>    \n",
    "    <li>\n",
    "        National Institute of Standards and Technology - \n",
    "        <a href=\"https://bit.ly/2J75EyX\">\n",
    "        Weighted Least Squares Regression</a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        Carnegie Mellon University - Cosma Shalizi - \n",
    "        <a href=\"https://bit.ly/2IICB5y\">\n",
    "        Extending Linear Regression: Weighted Least Squares, Heteroskedasticity, Local Polynomial Regression</a>\n",
    "    </li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
